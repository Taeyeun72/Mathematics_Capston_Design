{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe60cd1c",
   "metadata": {},
   "source": [
    "# Glow-TTS + HiFi-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ee061",
   "metadata": {},
   "source": [
    "- Glow-TTS와 HiFi-GAN을 결합한 TTS 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d91f599",
   "metadata": {},
   "source": [
    "# 0. Import Libraries & Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f2108",
   "metadata": {},
   "source": [
    "## 0.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dd940c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Glow-TTS\n",
    "\"\"\"\n",
    "# 1. Data\n",
    "import torch\n",
    "import numpy as np\n",
    "import os, glob, librosa, re, scipy\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# 2. Model\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "import math\n",
    "import time\n",
    "\n",
    "# 3. Training\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "HiFi-GAN\n",
    "\"\"\"\n",
    "# 2.1. Generator\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm, spectral_norm\n",
    "import time\n",
    "\n",
    "# 3. Training\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Inference\n",
    "from jamo import hangul_to_jamo\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70811b1f",
   "metadata": {},
   "source": [
    "## 0.2. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d1eaae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glow-TTS Major Hyperparameters\n",
    "batch_size = 32\n",
    "logging_step = 10\n",
    "validation_step = 100\n",
    "checkpoint_step = 1000\n",
    "\n",
    "# HiFi-GAN Major Hyperparameters\n",
    "hifi_batch_size = 4\n",
    "\n",
    "# Hyperparameters for Generating Mel-spectrogram \n",
    "sample_rate = 22050\n",
    "preemphasis = 0.97\n",
    "n_fft = 1024\n",
    "hop_length = 256\n",
    "win_length = 1024\n",
    "ref_db = 20\n",
    "max_db = 100\n",
    "mel_dim = 80\n",
    "\n",
    "# 2.1. Encoder\n",
    "symbol_length = 73 # len(symbols) = 70 (PAD + EOS + VALID_CHARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f8f19",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fbb740",
   "metadata": {},
   "source": [
    "- Data는 HiFi-GAN에서 전처리한 데이터를 그대로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e28a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '_'\n",
    "EOS = '~'\n",
    "SPACE = ' '\n",
    "\n",
    "JAMO_LEADS = \"\".join([chr(_) for _ in range(0x1100, 0x1113)])\n",
    "JAMO_VOWELS = \"\".join([chr(_) for _ in range(0x1161, 0x1176)])\n",
    "JAMO_TAILS = \"\".join([chr(_) for _ in range(0x11A8, 0x11C3)])\n",
    "ETC = \".!?\"\n",
    "\n",
    "VALID_CHARS = JAMO_LEADS + JAMO_VOWELS + JAMO_TAILS + SPACE + ETC\n",
    "symbols = PAD + EOS + VALID_CHARS\n",
    "\n",
    "_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
    "_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n",
    "\n",
    "# text를 초성, 중성, 종성으로 분리하여 id로 반환하는 함수\n",
    "def text_to_sequence(text):\n",
    "    sequence = []\n",
    "    if not 0x1100 <= ord(text[0]) <= 0x1113:\n",
    "        text = ''.join(list(hangul_to_jamo(text)))\n",
    "    for s in text:\n",
    "        sequence.append(_symbol_to_id[s])\n",
    "    sequence.append(_symbol_to_id['~'])\n",
    "    return sequence\n",
    "\n",
    "def sequence_to_text(sequence):\n",
    "    result = ''\n",
    "    for symbol_id in sequence:\n",
    "        if symbol_id in _id_to_symbol:\n",
    "            s = _id_to_symbol[symbol_id]\n",
    "            result += s\n",
    "    return result.replace('}{', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "270fbd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"C:/Users/Poco/Jupyter/PaperReview/dataset/ty/data\"\n",
    "\n",
    "class TextMelDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.text_list = sorted(glob.glob(os.path.join(data_dir + '/text', '*.npy')))\n",
    "        self.mel_list = sorted(glob.glob(os.path.join(data_dir + '/mel', '*.npy')))\n",
    "        self.wav_list = sorted(glob.glob(os.path.join(data_dir + '/wav', '*.npy')))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.from_numpy(np.load(self.text_list[idx]))\n",
    "        text_len = len(text)\n",
    "        \n",
    "        mel = torch.from_numpy(np.load(self.mel_list[idx]))\n",
    "        mel_len = mel.shape[0]\n",
    "        \n",
    "        wav = torch.from_numpy(np.load(self.wav_list[idx]))\n",
    "        wav_len = wav.shape[0]\n",
    "        return (text, text_len, mel, mel_len, wav, wav_len)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    text = []\n",
    "    text_len = []\n",
    "    mel = []\n",
    "    mel_len = []\n",
    "    wav = []\n",
    "    wav_len = []\n",
    "    \n",
    "    for t, tl, m, ml, w, wl in batch:\n",
    "        text.append(t)\n",
    "        text_len.append(tl)\n",
    "        mel.append(m)\n",
    "        mel_len.append(ml)\n",
    "        wav.append(w)\n",
    "        wav_len.append(wl)\n",
    "        \n",
    "    max_text_len = max(text_len)\n",
    "    max_mel_len = max(mel_len)\n",
    "    max_wav_len = max(wav_len)\n",
    "    \n",
    "    # text zero_padding\n",
    "    padded_text_batch = torch.zeros((len(batch), max_text_len), dtype=torch.int32)\n",
    "    for i, x in enumerate(text):\n",
    "        padded_text_batch[i, :len(x)] = torch.Tensor(x)\n",
    "    \n",
    "    # mel zero_padding\n",
    "    padded_mel_batch = torch.zeros((len(batch), max_mel_len, mel_dim), dtype=torch.float32)\n",
    "    for i, x in enumerate(mel):\n",
    "        padded_mel_batch[i, :x.shape[0], :x.shape[1]] = torch.Tensor(x)\n",
    "        \n",
    "    # wav zero_padding\n",
    "    padded_wav_batch = torch.zeros((len(batch), max_wav_len), dtype=torch.float32)\n",
    "    for i, x in enumerate(wav):\n",
    "        padded_wav_batch[i, :x.shape[0]] = torch.Tensor(x)\n",
    "        \n",
    "    return padded_text_batch, text_len, padded_mel_batch, mel_len, padded_wav_batch, wav_len\n",
    "\n",
    "dataset = TextMelDataset(data_dir)\n",
    "\n",
    "# Split dataset into training and validation\n",
    "dataset_size = len(dataset)\n",
    "train_size = 3000\n",
    "indices = list(range(dataset_size))\n",
    "\n",
    "# 데이터 인덱스를 섞지 않고 앞에서부터 train_size만큼을 train_indices로 선택\n",
    "train_indices = indices[:train_size]\n",
    "# 나머지 인덱스를 val_indices로 선택\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "# 인덱스 기반으로 데이터 세트 분할\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "\n",
    "\"\"\"\n",
    "# 랜덤하게 split하는 방법은 아래와 같다.\n",
    "train_ratio = 0.99\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\"\"\"\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True,\n",
    "                              batch_size=hifi_batch_size, collate_fn=collate_fn,\n",
    "                              pin_memory=True) # Glow-TTS는 batch_size, hifi-GAN은 hifi_batch_size를 이용하라.\n",
    "val_dataloader = DataLoader(val_dataset,  shuffle=False,\n",
    "                            batch_size=1, collate_fn=collate_fn,\n",
    "                            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b72a042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[0](text) shape(B, Max_T): torch.Size([4, 98])\n",
      "batch[1](text_len) len(B): 4\n",
      "batch[2](mel) shape(B, Max_F, mel_dim): torch.Size([4, 445, 80])\n",
      "batch[3](mel_len) len(B): 4\n",
      "batch[4](wav) shape(B, Max_L): torch.Size([4, 113934])\n",
      "batch[5](wav_len) len(B): 4\n",
      "Total: 3050\n",
      "num_of_batches: 750\n"
     ]
    }
   ],
   "source": [
    "# DataLoader 객체를 반복자로 변환\n",
    "dataiter = iter(train_dataloader)\n",
    "\n",
    "# 데이터 한 번 추출\n",
    "batch = next(dataiter)\n",
    "\n",
    "print('batch[0](text) shape(B, Max_T):', batch[0].shape) # Max_T: The number of text length\n",
    "print('batch[1](text_len) len(B):', len(batch[1]))\n",
    "print('batch[2](mel) shape(B, Max_F, mel_dim):', batch[2].shape) # Max_F: The number of Mel-spectrogram frames\n",
    "print('batch[3](mel_len) len(B):', len(batch[3]))\n",
    "print('batch[4](wav) shape(B, Max_L):', batch[4].shape) # Max_L: length of time(seconds) * sampling_rate(22050)\n",
    "print('batch[5](wav_len) len(B):', len(batch[5]))\n",
    "print('Total:', dataset.__len__())\n",
    "print('num_of_batches:', len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21afeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[0](text) shape(B, Max_T): torch.Size([1, 102])\n",
      "batch[1](text_len) len(B): 1\n",
      "batch[2](mel) shape(B, Max_F, mel_dim): torch.Size([1, 470, 80])\n",
      "batch[3](mel_len) len(B): 1\n",
      "batch[4](wav) shape(B, Max_L): torch.Size([1, 120334])\n",
      "batch[5](wav_len) len(B): 1\n",
      "Total: 3050\n",
      "num_of_batches: 50\n"
     ]
    }
   ],
   "source": [
    "# DataLoader 객체를 반복자로 변환\n",
    "dataiter = iter(val_dataloader)\n",
    "\n",
    "# 데이터 한 번 추출\n",
    "batch = next(dataiter)\n",
    "\n",
    "print('batch[0](text) shape(B, Max_T):', batch[0].shape) # Max_T: The number of text length\n",
    "print('batch[1](text_len) len(B):', len(batch[1]))\n",
    "print('batch[2](mel) shape(B, Max_F, mel_dim):', batch[2].shape) # Max_F: The number of Mel-spectrogram frames\n",
    "print('batch[3](mel_len) len(B):', len(batch[3]))\n",
    "print('batch[4](wav) shape(B, Max_L):', batch[4].shape) # Max_L: length of time(seconds) * sampling_rate(22050)\n",
    "print('batch[5](wav_len) len(B):', len(batch[5]))\n",
    "print('Total:', dataset.__len__())\n",
    "print('num_of_batches:', len(val_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94651d85",
   "metadata": {},
   "source": [
    "# 2. Glow-TTS Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6940a5f",
   "metadata": {},
   "source": [
    "## 2.1. Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8690cedc",
   "metadata": {},
   "source": [
    "### 2.1.1. Encoder Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46fb6f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    여러 곳에서 정규화(Norm)를 위해 사용되는 모듈.\n",
    "    \n",
    "    nn.LayerNorm이 이미 pytorch 안에 구현되어 있으나, 항상 마지막 차원을 정규화한다.\n",
    "    그래서 channel을 기준으로 정규화하는 LayerNorm을 따로 구현한다.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        channels: 입력 데이터의 channel 수 | LayerNorm은 channel 차원을 정규화한다.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.eps = 1e-4\n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.ones(channels)) # 학습 가능한 파라미터\n",
    "        self.beta = nn.Parameter(torch.zeros(channels)) # 학습 가능한 파라미터\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, channels, *) | 정규화할 입력 데이터\n",
    "        =====outputs=====\n",
    "        x: (B, channels, *) | channel 차원이 정규화된 데이터\n",
    "        \"\"\"\n",
    "        mean = torch.mean(x, dim=1, keepdim=True) # channel 차원(index=1)의 평균 계산, 차원을 유지한다.\n",
    "        variance = torch.mean((x-mean)**2, dim=1, keepdim=True) # 분산 계산\n",
    "        \n",
    "        x = (x - mean) * (variance + self.eps)**(-0.5) # (x - m) / sqrt(v)\n",
    "        \n",
    "        n = len(x.shape)\n",
    "        shape = [1] * n\n",
    "        shape[1] = -1 # shape = [1, -1, 1] or [1, -1, 1, 1]\n",
    "        x = x * self.gamma.view(*shape) + self.beta.view(*shape) # y = x*gamma + beta\n",
    "        \n",
    "        return x\n",
    "\n",
    "class PreNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder의 1번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        for i in range(3):\n",
    "            self.convs.append(nn.Conv1d(192, 192, kernel_size=5, padding=2)) # (B, 192, T) 유지\n",
    "            self.norms.append(LayerNorm(192)) # (B, 192, T) 유지\n",
    "        self.linear = nn.Conv1d(192, 192, kernel_size=1) # (B, 192, T) 유지 | linear 역할을 하는 conv\n",
    "        \n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 192, T) | Embedding된 입력 데이터\n",
    "        x_mask: (B, 1, T) | 글자 길이에 따른 mask (글자가 있으면 True, 없으면 False로 구성)\n",
    "        =====outputs=====\n",
    "        x: (B, 192, T)\n",
    "        \"\"\"\n",
    "        x0 = x\n",
    "        for i in range(3):\n",
    "            x = self.convs[i](x * x_mask)\n",
    "            x = self.norms[i](x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        x = x0 + x # residual connection\n",
    "        return x\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder 중 2번째 모듈인 TransformerEncoder의 1번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_heads = 2\n",
    "        self.window_size = 4\n",
    "        self.k_channels = 192 // self.n_heads # 96\n",
    "        \n",
    "        self.linear_q = nn.Conv1d(192, 192, kernel_size=1) # (B, 192, T) 유지\n",
    "        self.linear_k = nn.Conv1d(192, 192, kernel_size=1) # (B, 192, T) 유지\n",
    "        self.linear_v = nn.Conv1d(192, 192, kernel_size=1) # (B, 192, T) 유지\n",
    "        nn.init.xavier_uniform_(self.linear_q.weight)\n",
    "        nn.init.xavier_uniform_(self.linear_k.weight)\n",
    "        nn.init.xavier_uniform_(self.linear_v.weight)\n",
    "        \n",
    "        relative_std = self.k_channels ** (-0.5) # 0.1xx\n",
    "        self.relative_k = nn.Parameter(torch.randn(1, self.window_size * 2 + 1, self.k_channels) * relative_std) # (1, 9, 96)\n",
    "        self.relative_v = nn.Parameter(torch.randn(1, self.window_size * 2 + 1, self.k_channels) * relative_std) # (1, 9, 96)\n",
    "        \n",
    "        self.attention_weights = None\n",
    "        self.linear_out = nn.Conv1d(192, 192, kernel_size=1) # (B, 192, T) 유지\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, query, context, attention_mask, self_attention=True):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        query: (B, 192, T_target) | Glow-TTS에서는 self-attention만 이용하므로 query와 context가 동일한 텐서 x이다.\n",
    "        context: (B, 192, T_source) | query = context || 여기에서는 특히 T_source = T_target 이다.\n",
    "        attention_mask: (B, 1, T, T) | x_mask.unsqueeze(2) * z_mask.unsqueeze(3)\n",
    "        self_attention: True/False | self_attention일 때 relative position representations를 적용한다. 여기에서는 항상 True이다.\n",
    "        # 실제로는 query와 context에 같은 텐서 x를 입력하면 된다.\n",
    "        =====outputs=====\n",
    "        output: (B, 192, T)\n",
    "        \"\"\"\n",
    "        \n",
    "        query = self.linear_q(query)\n",
    "        key = self.linear_k(context)\n",
    "        value = self.linear_v(context)\n",
    "        \n",
    "        B, _, T_tar = query.size()\n",
    "        T_src = key.size(2)\n",
    "        query = query.view(B, self.n_heads, self.k_channels, T_tar).transpose(2, 3)\n",
    "        key = key.view(B, self.n_heads, self.k_channels, T_src).transpose(2, 3)\n",
    "        value = value.view(B, self.n_heads, self.k_channels, T_src).transpose(2, 3)\n",
    "            # (B, 192, T_src) -> (B, 2, 96, T_src) -> (B, 2, T_src, 96)\n",
    "            \n",
    "        scores = torch.matmul(query, key.transpose(2, 3)) / (self.k_channels ** 0.5)\n",
    "            # (B, 2, T_tar, 96) * (B, 2, 96, T_src) -> (B, 2, T_tar, T_src)\n",
    "        \n",
    "        if self_attention: # True\n",
    "            # Get relative embeddings (relative_keys) (1-1)\n",
    "            padding = max(T_src - (self.window_size + 1), 0) # max(T-5, 0)\n",
    "            start_pos = max((self.window_size + 1) - T_src, 0) # max(5-T, 0)\n",
    "            end_pos = start_pos + 2 * T_src - 1 # (2*T-1) or (T+4)\n",
    "            relative_keys = F.pad(self.relative_k, (0, 0, padding, padding))\n",
    "                # (1, 9, 96) -> (1, pad+9+pad, 96) = (1, 2T-1, 96)\n",
    "            \"\"\"\n",
    "            위 코드의 F.pad(input, pad) 에서 pad = (0, 0, padding, padding)은 다음을 의미한다.\n",
    "            - 앞의 (0, 0): input의 -1차원을 앞으로 0, 뒤로 0만큼 패딩한다.\n",
    "            - 앞의 (padding, padding): input의 -2차원을 앞으로 padding, 뒤로 padding만큼 패딩한다.\n",
    "            즉, F.pad에서 pad는 역순으로 생각해주어야 한다.\n",
    "            \"\"\"\n",
    "            relative_keys = relative_keys[:, start_pos:end_pos, :] # (1, 2T-1, 96)\n",
    "            \n",
    "            # Matmul with relative keys (2-1)\n",
    "            relative_keys = relative_keys.unsqueeze(0).transpose(2, 3) # (1, 2T-1, 96) -> (1, 1, 2T-1, 96) -> (1, 1, 96, 2T-1)\n",
    "            x = torch.matmul(query, relative_keys) # (B, 2, T_tar, 96) * (1, 1, 96, 2T_src-1) = (B, 2, T, 2T-1)\n",
    "                # self attention에서는 T_tar = T_src이므로 이를 다르게 고려할 필요가 없다.\n",
    "            \n",
    "            # Relative position to absolute position (3-1)\n",
    "            T = T_tar # Absolute position to relative position에서도 쓰임.\n",
    "            x = F.pad(x, (0, 1)) # (B, 2, T, 2*T-1) -> (B, 2, T, 2*T)\n",
    "            x = x.view(B, self.n_heads, T * 2 * T) # (B, 2, T, 2*T) -> (B, 2. 2T^2)\n",
    "            x = F.pad(x, (0, T-1)) # (B, 2, 2T^2 + T - 1)\n",
    "            x = x.view(B, self.n_heads, T+1, 2*T-1) # (B, 2, T+1, 2T-1)\n",
    "            relative_logits = x[:, :, :T, T-1:] # (B, 2, T, T)\n",
    "            \n",
    "            # Compute scores\n",
    "            scores_local = relative_logits / (self.k_channels ** 0.5)\n",
    "            scores = scores + scores_local # (B, 2, T, T)\n",
    "            \"\"\"\n",
    "            위 식은 Self-Attention with Relative Position Representations 논문의 5번 식을 구현한 것이다.\n",
    "            Relative- 논문: https://arxiv.org/pdf/1803.02155.pdf\n",
    "            \"\"\"\n",
    "\n",
    "        scores = scores.masked_fill(attention_mask == 0, -1e-4) # attention_mask가 0인 곳을 -1e-4로 채운다.\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1) # (B, 2, T_tar, T_src) # Relative- 논문에서의 alpha에 해당한다.\n",
    "        attention_weights = self.dropout(attention_weights) # dropout하는 이유가 무엇일까?\n",
    "        output = torch.matmul(attention_weights, value) # (B, 2, T_tar, T_src) * (B, 2, T_src, 96) -> (B, 2, T_tar, 96)\n",
    "        \n",
    "        if self_attention: # True\n",
    "            # Absolute position to relative position (3-2)\n",
    "            x = F.pad(attention_weights, (0, T-1)) # (B, 2, T, T) -> (B, 2, T, 2T-1)\n",
    "            x = x.view((B, self.n_heads, T * (2*T-1))) # (B, 2, 2T^2-T)\n",
    "            x = F.pad(x, (T, 0)) # (B, 2, 2T^2) # 앞에 패딩\n",
    "            x = x.view((B, self.n_heads, T, 2*T)) # (B, 2, T, 2T)\n",
    "            relative_weights = x[:, :, :, 1:] # (B, 2, T, 2T-1)\n",
    "            \n",
    "            # Get relative embeddings (relative_value) (1-2) # (1-1)과 거의 동일\n",
    "            padding = max(T_src - (self.window_size + 1), 0) # max(T-5, 0)\n",
    "            start_pos = max((self.window_size + 1) - T_src, 0) # max(5-T, 0)\n",
    "            end_pos = start_pos + 2 * T_src - 1 # (2*T-1) or (T+4)\n",
    "            relative_values = F.pad(self.relative_v, (0, 0, padding, padding))\n",
    "                # (1, 9, 96) -> (1, pad+9+pad, 96) = (1, 2T-1, 96)\n",
    "            relative_values = relative_values[:, start_pos:end_pos, :] # (1, 2T-1, 96)\n",
    "\n",
    "            # Matmul with relative values (2-2)\n",
    "            relative_values = relative_values.unsqueeze(0) # (1, 1, 2T-1, 96)\n",
    "\n",
    "            output = output + torch.matmul(relative_weights, relative_values)\n",
    "                # (B, 2, T, 2T-1) * (1, 1, 2T-1, 96) = (B, 2, T, 96)\n",
    "            \"\"\"\n",
    "            위 식은 Self-Attention with Relative Position Representations 논문의 3번 식을 구현한 것이다. (분배법칙 이용)\n",
    "            Relative- 논문: https://arxiv.org/pdf/1803.02155.pdf\n",
    "            \"\"\"\n",
    "        \n",
    "        output = output.transpose(2, 3).contiguous().view(B, 192, T_tar)\n",
    "            # (B, 2, 96, T) -> 메모리에 연속 배치 -> (B, 192, T)\n",
    "            \n",
    "        self.attention_weights = attention_weights # (B, 2, T, T)\n",
    "        output = self.linear_out(output)\n",
    "        return output # (B, 192, T)\n",
    "    \n",
    "class FFN(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder 중 2번째 모듈인 TransformerEncoder의 2번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(192, 768, kernel_size=3, padding=1) # (B, 192, T) -> (B, 768, T)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(768, 192, kernel_size=3, padding=1) # (B, 768, T) -> (B, 192, T)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 192, T)\n",
    "        x_mask: (B, 1, T)\n",
    "        =====outputs=====\n",
    "        output: (B, 192, T)\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "        output = x * x_mask\n",
    "        return output\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder의 2번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attentions = nn.ModuleList()\n",
    "        self.norms1 = nn.ModuleList()\n",
    "        self.ffns = nn.ModuleList()\n",
    "        self.norms2 = nn.ModuleList()\n",
    "        for i in range(6):\n",
    "            self.attentions.append(MultiHeadAttention())\n",
    "            self.norms1.append(LayerNorm(192))\n",
    "            self.ffns.append(FFN())\n",
    "            self.norms2.append(LayerNorm(192))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 192, T)\n",
    "        x_mask: (B, 1, T)\n",
    "        =====outputs=====\n",
    "        output: (B, 192, T)\n",
    "        \"\"\"\n",
    "        attention_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(3)\n",
    "            # (B, 1, 1, T) * (B, 1, T, 1) = (B, 1, T, T), only consist 0 or 1\n",
    "        for i in range(6):\n",
    "            x = x * x_mask\n",
    "            y = self.attentions[i](x, x, attention_mask)\n",
    "            y = self.dropout(y)\n",
    "            x = x + y # residual connection\n",
    "            x = self.norms1[i](x) # (B, 192, T) 유지\n",
    "            \n",
    "            y = self.ffns[i](x, x_mask)\n",
    "            y = self.dropout(y)\n",
    "            x = x + y # residual connection\n",
    "            x = self.norms2[i](x)\n",
    "        output = x * x_mask\n",
    "        return output # (B, 192, T)\n",
    "    \n",
    "class DurationPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder의 3번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(192, 256, kernel_size=3, padding=1) # (B, 192, T) -> (B, 256, T)\n",
    "        self.norm1 = LayerNorm(256)\n",
    "        self.conv2 = nn.Conv1d(256, 256, kernel_size=3, padding=1) # (B, 256, T) -> (B, 256, T)\n",
    "        self.norm2 = LayerNorm(256)\n",
    "        self.linear = nn.Conv1d(256, 1, kernel_size=1) # (B, 256, T) -> (B, 1, T)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 192, T)\n",
    "        x_mask: (B, 1, T)\n",
    "        =====outputs=====\n",
    "        output: (B, 1, T)\n",
    "        \"\"\"\n",
    "        x = self.conv1(x * x_mask) # (B, 192, T) -> (B, 256, T)\n",
    "        x = self.relu(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.conv2(x * x_mask) # (B, 256, T) -> (B, 256, T)\n",
    "        x = self.relu(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear(x * x_mask) # (B, 256, T) -> (B, 1, T)\n",
    "        output = x * x_mask\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5bcbfa",
   "metadata": {},
   "source": [
    "### 2.1.2. Main Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b687f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(symbol_length, 192) # (B, T) -> (B, T, 192)\n",
    "        nn.init.normal_(self.embedding.weight, 0.0, 192**(-0.5)) # 가중치 정규분포 초기화 (N(0, 0.07xx))\n",
    "        \n",
    "        self.prenet = PreNet()\n",
    "        self.transformer_encoder = TransformerEncoder()\n",
    "        self.project_mean = nn.Conv1d(192, 80, kernel_size=1) # (B, 192, T) -> (B, 80, T)\n",
    "        self.project_std = nn.Conv1d(192, 80, kernel_size=1) # (B, 192, T) -> (B, 80, T)\n",
    "        \n",
    "        self.duration_predictor = DurationPredictor()\n",
    "        \n",
    "    def forward(self, text, text_len):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        text: (B, Max_T)\n",
    "        text_len: (B)\n",
    "        =====outputs=====\n",
    "        x_mean: (B, 80, T) | 평균, 논문 저자 구현의 train.py에서 out_channels를 80으로 설정한 것을 알 수 있음.\n",
    "        x_std: (B, 80, T) | 표준편차\n",
    "        x_dur: (B, 1, T)\n",
    "        x_mask: (B, 1, T)\n",
    "        \"\"\"\n",
    "        x = self.embedding(text) * math.sqrt(192) # (B, T) -> (B, T, 192) # math.sqrt(192) = 13.xx (수정)\n",
    "        x = x.transpose(1, 2) # (B, T, 192) -> (B, 192, T)\n",
    "        \n",
    "        # Make the x_mask\n",
    "        x_mask = torch.zeros_like(x[:, 0:1, :], dtype=torch.bool) # (B, 1, T)\n",
    "        for idx, length in enumerate(text_len):\n",
    "            x_mask[idx, :, :length] = True\n",
    "        \n",
    "        x = self.prenet(x, x_mask) # (B, 192, T)\n",
    "        x = self.transformer_encoder(x, x_mask) # (B, 192, T)\n",
    "        \n",
    "        # project\n",
    "        x_mean = self.project_mean(x) * x_mask # (B, 192, T) -> (B, 80, T)\n",
    "        # x_std = self.project_std(x) * x_mask # (B, 192, T) -> (B, 80, T)\n",
    "        ##### 아래는 mean_only를 적용한 것임. #####\n",
    "        x_std = torch.zeros_like(x_mean) # x_log_std: (B, 80, T), all zero # log std = 0이므로 std = 1로 계산됨.\n",
    "\n",
    "        # duration predictor\n",
    "        x_dp = torch.detach(x) # stop_gradient\n",
    "        x_dur = self.duration_predictor(x_dp, x_mask) # (B, 192, T) -> (B, 1, T)\n",
    "        \n",
    "        return x_mean, x_std, x_dur, x_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a15a880",
   "metadata": {},
   "source": [
    "## 2.2. Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d97cbc",
   "metadata": {},
   "source": [
    "### 2.2.1. Decoder Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bfdd2b",
   "metadata": {},
   "source": [
    "- 필요성을 확인할 수 없기에 다음을 무시하고 구현한다. 나중에 필요할 때 추가로 구현하라.\n",
    "    - g: global condition (n_speaker)\n",
    "    - ddi (데이터 종속적 초기화(data-dependent initialization))\n",
    "    - no_jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c03168ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Decoder는 Glow: Generative Flow with Invertible 1×1 Convolutions 논문의 기본 구조를 따라간다.\n",
    "Glow 논문: https://arxiv.org/pdf/1807.03039.pdf\n",
    "\"\"\"\n",
    "def Squeeze(x, x_mask):\n",
    "    \"\"\"\n",
    "    Decoder의 preprocessing\n",
    "    =====inputs=====\n",
    "    x: (B, 80, F) | mel_spectrogram or latent representation\n",
    "    x_mask: (B, 1, F)\n",
    "    =====outputs=====\n",
    "    x: (B, 160, F//2) | F//2 = [F/2] ([]: 가우스 기호)\n",
    "    x_mask: (B, 160, F//2)\n",
    "    \"\"\"\n",
    "    B, C, F = x.size()\n",
    "    x = x[:, :, :(F//2)*2] # F가 홀수이면 맨 뒤 한 frame을 버림.\n",
    "    x = x.view(B, C, F//2, 2) # (B, 80, F//2, 2)\n",
    "    x = x.permute(0, 3, 1, 2).contiguous() # (B, 2, 80, F//2)\n",
    "    x = x.view(B, C*2, F//2) # (B, 160, F//2)\n",
    "    \n",
    "    x_mask = x_mask[:, :, 1::2] # (B, 1, F//2) frame을 1부터 한칸씩 건너뛴다.\n",
    "    x = x * x_mask # masking\n",
    "    return x, x_mask\n",
    "\n",
    "class ActNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder의 1번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.log_s = nn.Parameter(torch.zeros(1, 160, 1)) # Glow 논문의 s에서 log를 취한 것이다. 즉, log[s]\n",
    "        self.bias = nn.Parameter(torch.zeros(1, 160, 1))\n",
    "        \n",
    "    def forward(self, x, x_mask, reverse=False):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 160, F//2) | mel_spectrogram features\n",
    "        x_mask: (B, 1, F//2) | mel_spectrogram features의 mask. (Decoder의 Squeeze에서 변형됨.)\n",
    "        =====outputs=====\n",
    "        z: (B, 160, F//2)\n",
    "        log_det: (B) or None | log_determinant, reverse=True이면 None 반환\n",
    "        \"\"\"\n",
    "        x_len = torch.sum(x_mask, [1, 2]) # (B) | 1, 2차원의 값을 더한다. cf. [1, 2] 대신 [2]만 사용하면 shape가 (B, 1)이 된다.\n",
    "        \n",
    "        if not reverse:\n",
    "            z = (x * torch.exp(self.log_s) + self.bias) * x_mask # function & masking\n",
    "            log_det = x_len * torch.sum(self.log_s) # log_determinant\n",
    "                # Glow 논문의 Table 1을 확인하라. log_s를 log[s]라 볼 수 있다.\n",
    "                # determinant 대신 log_determinant를 사용하는 이유는 det보다 작은 수치와 적은 계산량 때문으로 추측된다.\n",
    "        else:\n",
    "            z = ((x - self.bias) / torch.exp(self.log_s)) * x_mask # inverse function & masking\n",
    "            log_det = None\n",
    "        \n",
    "        return z, log_det\n",
    "\n",
    "class InvertibleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder의 2번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        Q = torch.linalg.qr(torch.FloatTensor(4, 4).normal_())[0] # (4, 4)\n",
    "        \"\"\"\n",
    "        torch.FloatTensor(4, 4).normal_(): 정규분포 N(0, 1)에서 무작위로 추출한 4x4 matrix\n",
    "        Q, R = torch.linalg.qr(W): QR분해 | Q: 직교 행렬, R: upper traiangular 행렬 cf. det(Q) = 1 or -1\n",
    "        \"\"\"\n",
    "        if torch.det(Q) < 0:\n",
    "            Q[:, 0] = -1 * Q[:, 0] # 0번째 열의 부호를 바꿔서 det(Q) = -1로 만든다.\n",
    "        self.W = nn.Parameter(Q)\n",
    "    \n",
    "    def forward(self, x, x_mask, reverse=False):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 160, F//2)\n",
    "        x_mask: (B, 1, F//2)\n",
    "        =====outputs=====\n",
    "        z: (B, 160, F//2)\n",
    "        log_det: (B) or None\n",
    "        \"\"\"\n",
    "        B, C, f = x.size() # B, 160, F//2\n",
    "        x_len = torch.sum(x_mask, [1, 2]) # (B)\n",
    "        \n",
    "        # channel mixing\n",
    "        x = x.view(B, 2, C//4, 2, f) # (B, 2, 40, 2, F//2)\n",
    "        x = x.permute(0, 1, 3, 2, 4).contiguous() # (B, 2, 2, 40, F//2)\n",
    "        x = x.view(B, 4, C//4, f) # (B, 4, 40, F//2)\n",
    "        \n",
    "        # 편의상 log_det부터 구한다.\n",
    "        if not reverse:\n",
    "            weight = self.W\n",
    "            log_det = (C/4) * x_len * torch.logdet(self.W) # (B) | torch.logdet(W): log(det(W))\n",
    "                # height = C/4, width = x_len 인 상황임을 고려하면 Glow 논문의 log_determinant 식과 같다.\n",
    "        else:\n",
    "            weight = torch.linalg.inv(self.W) # inverse matrix\n",
    "            log_det = None\n",
    "        \n",
    "        weight = weight.view(4, 4, 1, 1)\n",
    "        z = F.conv2d(x, weight) # (B, 4, 40, F//2) * (4, 4, 1, 1) -> (B, 4, 40, F//2)\n",
    "        \"\"\"\n",
    "        F.conv2d(x, weight)의 convolution 연산은 다음과 같이 생각해야 한다.\n",
    "        (B, 4, 40, F//2): (batch_size, in_channels, height, width)\n",
    "        (4, 4, 1, 1): (out_channels, in_channels/groups, kernel_height, kernel_width)\n",
    "        \n",
    "        즉, nn.Conv2d(4, 4, kernel_size=(1, 1))인 상황에 가중치를 준 것이다.\n",
    "        \"\"\"\n",
    "        \n",
    "        # channel unmixing\n",
    "        z = z.view(B, 2, 2, C//4, f) # (B, 4, 40, F//2) -> (B, 2, 2, 40, F//2)\n",
    "        z = z.permute(0, 1, 3, 2, 4).contiguous() # (B, 2, 40, 2, F//2)\n",
    "        z = z.view(B, C, f) * x_mask # (B, 160, F//2) & masking\n",
    "        return z, log_det\n",
    "    \n",
    "class WN(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder의 3번째 모듈인 AffineCouplingLayer의 모듈\n",
    "    \n",
    "    해당 구조는 WAVEGLOW: A FLOW-BASED GENERATIVE NETWORK FOR SPEECH SYNTHESIS 로부터 제안되었다.\n",
    "    WaveGlow 논문: https://arxiv.org/pdf/1811.00002.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, dilation_rate=1):\n",
    "        super().__init__()\n",
    "        self.in_layers = nn.ModuleList()\n",
    "        self.res_skip_layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(4):\n",
    "            dilation = dilation_rate ** i # NVIDIA WaveGlow에서는 dilation_rate=2이지만, 여기에서는 1이므로 의미는 없다.\n",
    "            in_layer = weight_norm(nn.Conv1d(192, 2*192, kernel_size=5, dilation=dilation,\n",
    "                                 padding=((5-1) * dilation)//2)) # (B, 192, F//2) -> (B, 2*192, F//2)\n",
    "            self.in_layers.append(in_layer)\n",
    "            \n",
    "            if i < 3:\n",
    "                res_skip_layer = weight_norm(nn.Conv1d(192, 2*192, kernel_size=1)) # (B, 192, F//2) -> (B, 2*192, F//2)\n",
    "            else:\n",
    "                res_skip_layer = weight_norm(nn.Conv1d(192, 192, kernel_size=1)) # (B, 192, F//2) -> (B, 192, F//2)\n",
    "            self.res_skip_layers.append(res_skip_layer)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.05)\n",
    "    \n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 192, F//2)\n",
    "        x_mask: (B, 1, F//2)\n",
    "        =====outputs=====\n",
    "        output: (B, 192, F//2)\n",
    "        \"\"\"\n",
    "        output = torch.zeros_like(x) # (B, 192, F//2) all zeros\n",
    "        \n",
    "        for i in range(4):\n",
    "            x_in = self.in_layers[i](x) # (B, 192, F//2) -> (B, 2*192, F//2)\n",
    "            x_in = self.dropout(x_in) # dropout\n",
    "            \n",
    "            # fused add tanh sigmoid multiply\n",
    "            tanh_act = torch.tanh(x_in[:, :192, :]) # (B, 192, F//2)\n",
    "            sigmoid_act = torch.sigmoid(x_in[:, 192:, :]) # (B, 192, F//2)\n",
    "            \n",
    "            acts = sigmoid_act * tanh_act # (B, 192, F//2)\n",
    "            \n",
    "            x_out = self.res_skip_layers[i](acts) # (B, 192, F//2) -> (B, 2*192, F//2) or [last](B, 192, F//2)\n",
    "            if i < 3:\n",
    "                x = (x + x_out[:, :192, :]) * x_mask # residual connection & masking\n",
    "                output += x_out[:, 192:, :] # add output\n",
    "            else:\n",
    "                output += x_out # (B, 192, F//2)\n",
    "        \n",
    "        output = output * x_mask # masking\n",
    "        return output\n",
    "\n",
    "class AffineCouplingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder의 3번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.start_conv = weight_norm(nn.Conv1d(160//2, 192, kernel_size=1)) # (B, 80, F//2) -> (B, 192, F//2)\n",
    "        self.wn = WN()\n",
    "        self.end_conv = nn.Conv1d(192, 160, kernel_size=1) # (B, 192, F//2) -> (B, 160, F//2)\n",
    "        # end_conv의 초기 가중치를 0으로 설정하는 것이 처음에 학습하지 않는 역할을 하며, 이는 학습 안정화에 도움이 된다.\n",
    "        self.end_conv.weight.data.zero_() # weight를 0으로 초기화\n",
    "        self.end_conv.bias.data.zero_() # bias를 0으로 초기화\n",
    "        \n",
    "    def forward(self, x, x_mask, reverse=False):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 160, F//2)\n",
    "        x_mask: (B, 1, F//2)\n",
    "        =====outputs=====\n",
    "        z: (B, 160, F//2)\n",
    "        log_det: (B) or None\n",
    "        \"\"\"\n",
    "        B, C, f = x.size() # B, 160, F//2\n",
    "        x_0, x_1 = x[:, :C//2, :], x[:, C//2:, :] # split: (B, 80, F//2) x2\n",
    "        \n",
    "        x = self.start_conv(x_0) * x_mask # (B, 80, F//2) -> (B, 192, F//2) & masking\n",
    "        x = self.wn(x, x_mask) # (B, 192, F//2)\n",
    "        out = self.end_conv(x) # (B, 192, F//2) -> (B, 160, F//2)\n",
    "        \n",
    "        z_0 = x_0 # (B, 80, F//2)\n",
    "        m = out[:, :C//2, :] # (B, 80, F//2)\n",
    "        log_s = out[:, C//2:, :] # (B, 80, F//2)\n",
    "        \n",
    "        if not reverse:\n",
    "            z_1 = (torch.exp(log_s) * x_1 + m) * x_mask # (B, 80, F//2) | function & masking \n",
    "            log_det = torch.sum(log_s * x_mask, [1, 2]) # (B)\n",
    "        else:\n",
    "            z_1 = (x_1 - m) / torch.exp(log_s) * x_mask # (B, 80, F//2) | inverse function & masking\n",
    "            log_det = None\n",
    "        \n",
    "        z = torch.cat([z_0, z_1], dim=1) # (B, 160, F//2)\n",
    "        return z, log_det\n",
    "    \n",
    "def Unsqueeze(x, x_mask):\n",
    "    \"\"\"\n",
    "    Decoder의 postprocessing\n",
    "    =====inputs=====\n",
    "    x: (B, 160, F//2)\n",
    "    x_mask: (B, 1, F//2)\n",
    "    =====outputs=====\n",
    "    x: (B, 80, F)\n",
    "    x_mask: (B, 1, F)\n",
    "    \"\"\"\n",
    "    B, C, f = x.size() # B, 160, F//2\n",
    "    x = x.view(B, 2, C//2, f) # (B, 2, 80, F//2)\n",
    "    x = x.permute(0, 2, 3, 1).contiguous() # (B, 80, F//2, 2)\n",
    "    x = x.view(B, C//2, 2*f) # (B, 160, F)\n",
    "    \n",
    "    x_mask = x_mask.unsqueeze(3).repeat(1, 1, 1, 2).view(B, 1, 2*f) # (B, 1, F//2, 1) -> (B, 1, F//2, 2) -> (B, 1, F)\n",
    "    x = x * x_mask # masking\n",
    "    return x, x_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba03d9",
   "metadata": {},
   "source": [
    "### 2.2.2. Main Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74f4689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flows = nn.ModuleList()\n",
    "        for i in range(12):\n",
    "            self.flows.append(ActNorm())\n",
    "            self.flows.append(InvertibleConv())\n",
    "            self.flows.append(AffineCouplingLayer())\n",
    "        \n",
    "    def forward(self, x, x_mask, reverse=False):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 80, F) | mel-spectrogram(Direct) OR latent representation(Reverse)\n",
    "        x_mask: (B, 1, F)\n",
    "        =====outputs=====\n",
    "        z: (B, 80, F) | latent representation(Direct) OR mel-spectrogram(Reverse)\n",
    "        total_log_det: (B) or None | log determinant\n",
    "        \"\"\"\n",
    "        if not reverse:\n",
    "            flows = self.flows\n",
    "            total_log_det = 0\n",
    "        else:\n",
    "            flows = reversed(self.flows)\n",
    "            total_log_det = None\n",
    "        \n",
    "        x, x_mask = Squeeze(x, x_mask) # (B, 80, F) -> (B, 160, F//2) | (B, 1, F) -> (B, 1, F//2)\n",
    "        \n",
    "        for f in flows:\n",
    "            if not reverse:\n",
    "                x, log_det = f(x, x_mask, reverse=reverse)\n",
    "                total_log_det += log_det\n",
    "            else:\n",
    "                x, _ = f(x, x_mask, reverse=reverse)\n",
    "                \n",
    "        x, x_mask = Unsqueeze(x, x_mask) # (B, 160, F//2) -> (B, 80, F) | (B, 1, F//2) -> (B, 1, F)\n",
    "        \n",
    "        return x, total_log_det"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdeb2f0",
   "metadata": {},
   "source": [
    "## 2.3. Generator (Encoder+Decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ee5bd",
   "metadata": {},
   "source": [
    "### 2.3.1. Generator Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93b25065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAS(path, logp, T_max, F_max):\n",
    "    \"\"\"\n",
    "    Glow-TTS의 모듈인 maximum_path의 모듈\n",
    "    MAS 알고리즘을 수행하는 함수이다.\n",
    "    =====inputs=====\n",
    "    path: (T, F)\n",
    "    logp: (T, F)\n",
    "    T_max: (1)\n",
    "    F_max: (1)\n",
    "    =====outputs=====\n",
    "    path: (T, F) | 0과 1로 구성된 alignment\n",
    "    \"\"\"\n",
    "    neg_inf = -1e9 # negative infinity\n",
    "    # forward\n",
    "    for j in range(F_max):\n",
    "        for i in range(max(0, T_max + j - F_max), min(T_max, j + 1)): # 평행사변형을 생각하라.\n",
    "            # Q_i_j-1 (current)\n",
    "            if i == j:\n",
    "                Q_cur = neg_inf\n",
    "            else:\n",
    "                Q_cur = logp[i, j-1] # j=0이면 i도 0이므로 j-1을 사용해도 된다.\n",
    "            \n",
    "            # Q_i-1_j-1 (previous)\n",
    "            if i==0:\n",
    "                if j==0:\n",
    "                    Q_prev = 0. # i=0, j=0인 경우에는 logp 값만 반영해야 한다.\n",
    "                else:\n",
    "                    Q_prev = neg_inf # i=0인 경우에는 Q_i-1_j-1을 반영하지 않아야 한다.\n",
    "            else:\n",
    "                Q_prev = logp[i-1, j-1]\n",
    "            \n",
    "            # logp에 Q를 갱신한다.\n",
    "            logp[i, j] = max(Q_cur, Q_prev) + logp[i, j]\n",
    "    \n",
    "    # backtracking\n",
    "    idx = T_max - 1\n",
    "    for j in range(F_max-1, -1, -1): # F_max-1부터 -1까지(-1 포함 없이 0까지) -1씩 감소\n",
    "        path[idx, j] = 1\n",
    "        if idx != 0:\n",
    "            if (logp[idx, j-1] < logp[idx-1, j-1]) or (idx == j):\n",
    "                idx -= 1\n",
    "    \n",
    "    return path\n",
    "        \n",
    "\n",
    "def maximum_path(logp, attention_mask):\n",
    "    \"\"\"\n",
    "    Glow-TTS에 사용되는 모듈\n",
    "    MAS를 사용하여 alignment를 찾아주는 역할을 한다.\n",
    "    논문 저자 구현에서는 cpython을 이용하여 병렬 처리를 구현한 듯 하나\n",
    "    여기에서는 python만을 이용하여 구현하였다.\n",
    "    =====inputs=====\n",
    "    logp: (B, T, F) | N(x_mean, x_std)의 log-likelihood\n",
    "    attention_mask: (B, T, F)\n",
    "    =====outputs=====\n",
    "    path: (B, T, F) | alignment\n",
    "    \"\"\"\n",
    "    B = logp.shape[0]\n",
    "    \n",
    "    logp = logp * attention_mask\n",
    "    # 계산은 CPU에서 실행되도록 하기 위해 기존의 device를 저장하고 .cpu().numpy()를 한다.\n",
    "    logp_device = logp.device\n",
    "    logp_type = logp.dtype\n",
    "    logp = logp.data.cpu().numpy().astype(np.float32)\n",
    "    attention_mask = attention_mask.data.cpu().numpy()\n",
    "    \n",
    "    path = np.zeros_like(logp).astype(np.int32) # (B, T, F)\n",
    "    T_max = attention_mask.sum(1)[:, 0].astype(np.int32) # (B)\n",
    "    F_max = attention_mask.sum(2)[:, 0].astype(np.int32) # (B)\n",
    "    \n",
    "    # MAS 알고리즘\n",
    "    for idx in range(B):\n",
    "        path[idx] = MAS(path[idx], logp[idx], T_max[idx], F_max[idx]) # (T, F)\n",
    "    return torch.from_numpy(path).to(device=logp_device, dtype=logp_type)\n",
    "\n",
    "def generate_path(ceil_dur, attention_mask):\n",
    "    \"\"\"\n",
    "    Glow-TTS에 사용되는 모듈\n",
    "    inference 과정에서 alignment를 만들어낸다.\n",
    "    =====input=====\n",
    "    ceil_dur: (B, T) | 추론한 duration에 ceil 연산한 것 | ex) [[2, 1, 2, 2, ...], [1, 2, 1, 3, ...], ...]\n",
    "    attention_mask: (B, T, F)\n",
    "    =====output=====\n",
    "    path: (B, T, F) | alignment\n",
    "    \"\"\"\n",
    "    B, T, Frame = attention_mask.shape\n",
    "    cum_dur = torch.cumsum(ceil_dur, 1)\n",
    "    cum_dur = cum_dur.to(torch.int32) # (B, T) | 누적합 | ex) [[2, 3, 5, 7, ...], [1, 3, 4, 7, ...], ...]\n",
    "    path = torch.zeros(B, T, Frame).to(ceil_dur.device) # (B, T, F) | all False(0)\n",
    "    \n",
    "    # make the sequence_mask\n",
    "    for b, batch_cum_dur in enumerate(cum_dur):\n",
    "        for t, each_cum_dur in enumerate(batch_cum_dur):\n",
    "            path[b, t, :each_cum_dur] = torch.ones((1, 1, each_cum_dur)).to(ceil_dur.device)\n",
    "                # cum_dur로부터 True(1)를 path에 새겨넣는다.\n",
    "    path = path - F.pad(path, (0, 0, 1, 0, 0, 0))[:, :-1] # (B, T, F)\n",
    "    \"\"\"\n",
    "    ex) batch를 잠시 제외해두고 예시를 든다.\n",
    "    [[1, 1, 0, 0, 0, 0, 0],   [[0, 0, 0, 0, 0, 0, 0],    [[1, 1, 0, 0, 0, 0, 0],\n",
    "     [1, 1, 1, 0, 0, 0, 0], -  [1, 1, 0, 0, 0, 0, 0],  =  [0, 0, 1, 0, 0, 0, 0],\n",
    "     [1, 1, 1, 1, 1, 0, 0],    [1, 1, 1, 0, 0, 0, 0],     [0, 0, 0, 1, 1, 0, 0],\n",
    "     [1, 1, 1, 1, 1, 1, 1]]    [1, 1, 1, 1, 1, 0, 0]]     [0, 0, 0, 0, 0, 1, 1]]\n",
    "    \"\"\"\n",
    "    path = path * attention_mask\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fa32dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 아래 논문의 구현이 훨씬 빠르다. 이 논문 구현을 보고 위의 구현을 변경할 필요가 있다. #####\n",
    "def maximum_path(value, mask, max_neg_val=-np.inf):\n",
    "    \"\"\" Numpy-friendly version. It's about 4 times faster than torch version.\n",
    "    value: [b, t_x, t_y]\n",
    "    mask: [b, t_x, t_y]\n",
    "    \"\"\"\n",
    "    value = value * mask\n",
    "\n",
    "    device = value.device\n",
    "    dtype = value.dtype\n",
    "    value = value.cpu().detach().numpy()\n",
    "    mask = mask.cpu().detach().numpy().astype(bool)\n",
    "\n",
    "    b, t_x, t_y = value.shape\n",
    "    direction = np.zeros(value.shape, dtype=np.int64)\n",
    "    v = np.zeros((b, t_x), dtype=np.float32)\n",
    "    x_range = np.arange(t_x, dtype=np.float32).reshape(1,-1)\n",
    "    for j in range(t_y):\n",
    "        v0 = np.pad(v, [[0,0],[1,0]], mode=\"constant\", constant_values=max_neg_val)[:, :-1]\n",
    "        v1 = v\n",
    "        max_mask = (v1 >= v0)\n",
    "        v_max = np.where(max_mask, v1, v0)\n",
    "        direction[:, :, j] = max_mask\n",
    "\n",
    "        index_mask = (x_range <= j)\n",
    "        v = np.where(index_mask, v_max + value[:, :, j], max_neg_val)\n",
    "    direction = np.where(mask, direction, 1)\n",
    "\n",
    "    path = np.zeros(value.shape, dtype=np.float32)\n",
    "    index = mask[:, :, 0].sum(1).astype(np.int64) - 1\n",
    "    index_range = np.arange(b)\n",
    "    for j in reversed(range(t_y)):\n",
    "        path[index_range, index, j] = 1\n",
    "        index = index + direction[index_range, index, j] - 1\n",
    "    path = path * mask.astype(np.float32)\n",
    "    path = torch.from_numpy(path).to(device=device, dtype=dtype)\n",
    "    return path\n",
    "\n",
    "\n",
    "def generate_path(duration, mask):\n",
    "    \"\"\"\n",
    "    duration: [b, t_x]\n",
    "    mask: [b, t_x, t_y]\n",
    "    \"\"\"\n",
    "    device = duration.device\n",
    "\n",
    "    b, t_x, t_y = mask.shape # (B, T, F)\n",
    "    cum_duration = torch.cumsum(duration, 1) # 누적합, (B, T) \n",
    "    path = torch.zeros(b, t_x, t_y, dtype=mask.dtype).to(device=device) # (B, T, F)\n",
    "\n",
    "    cum_duration_flat = cum_duration.view(b * t_x) # (B*T)\n",
    "    path = sequence_mask(cum_duration_flat, t_y).to(mask.dtype) # (B*T, F)\n",
    "    path = path.view(b, t_x, t_y) # (B, T, F)\n",
    "    path = path.to(torch.float32)\n",
    "    path = path - F.pad(path, convert_pad_shape([[0, 0], [1, 0], [0, 0]]))[:,:-1] # (B, T, F) # T의 차원 맨 앞을 -1한다.\n",
    "    path = path * mask\n",
    "    return path\n",
    "\n",
    "def sequence_mask(length, max_length=None):\n",
    "    if max_length is None:\n",
    "        max_length = length.max()\n",
    "    x = torch.arange(max_length, dtype=length.dtype, device=length.device)\n",
    "    return x.unsqueeze(0) < length.unsqueeze(1)\n",
    "\n",
    "def convert_pad_shape(pad_shape):\n",
    "    l = pad_shape[::-1] # [[0, 0], [p, p], [0, 0]]\n",
    "    pad_shape = [item for sublist in l for item in sublist] # [0, 0, p, p, 0, 0]\n",
    "    return pad_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dfaa46",
   "metadata": {},
   "source": [
    "### 2.3.2. Main Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6664fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlowTTS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self, text, text_len, mel=None, mel_len=None, inference=False):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        text: (B, T)\n",
    "        text_len: (B) list\n",
    "        mel: (B, 80, F)\n",
    "        mel_len: (B) list\n",
    "        inference: True/False\n",
    "        =====outputs=====\n",
    "        (tuple) (z, z_mean, z_log_std, log_det, z_mask)\n",
    "            z(training) or y(inference): (B, 80, F) | z: latent representation, y: mel-spectrogram\n",
    "            z_mean: (B, 80, F)\n",
    "            z_log_std: (B, 80, F)\n",
    "            log_det: (B) or None\n",
    "            z_mask: (B, 1, F)\n",
    "        (tuple) (x_mean, x_log_std, x_mask)\n",
    "            x_mean: (B, 80, T)\n",
    "            x_log_std: (B, 80, T)\n",
    "            x_mask: (B, 1, T)\n",
    "        (tuple) (attention_alignment, x_log_dur, log_d)\n",
    "            attention_alignment: (B, T, F)\n",
    "            x_log_dur: (B, 1, T) | 추측한 duration의 log scale\n",
    "            log_d: (B, 1, T) | 적절하다고 추측한 alignment에서의 duration의 log scale\n",
    "        \"\"\"\n",
    "        x_mean, x_log_std, x_log_dur, x_mask = self.encoder(text, text_len)\n",
    "            # x_std, x_dur 에 log를 붙인 이유는, 논문 저자의 구현에서는 log가 취해진 값으로 간주하기 때문이다.\n",
    "        y, y_len = mel, mel_len\n",
    "        \n",
    "        if not inference: # training\n",
    "            y_max_len = y.size(2)\n",
    "        else: # inference\n",
    "            dur = torch.exp(x_log_dur) * x_mask # (B, 1, T)\n",
    "            ceil_dur = torch.ceil(dur) # (B, 1, T)\n",
    "            y_len = torch.clamp_min(torch.sum(ceil_dur, [1, 2]), 1).long() # (B)\n",
    "                # ceil_dur을 [1, 2] 축에 대해 sum한 뒤 최솟값이 1이상이 되도록 설정. 정수 long 타입으로 반환한다.\n",
    "            y_max_len = None\n",
    "        \n",
    "        # preprocessing\n",
    "        if y_max_len is not None:\n",
    "            y_max_len = (y_max_len // 2) * 2 # 홀수면 1을 빼서 짝수로 만든다.\n",
    "            y = y[:, :, :y_max_len] # y_max_len에 맞게 y를 조정\n",
    "            y_len = (y_len // 2) * 2 # y_len이 홀수이면 1을 빼서 짝수로 만든다.\n",
    "        \n",
    "        # make the z_mask\n",
    "        B = len(y_len)\n",
    "        temp_max = max(y_len)\n",
    "        z_mask = torch.zeros((B, 1, temp_max), dtype=torch.bool).to(device) # (B, 1, F)\n",
    "        for idx, length in enumerate(y_len):\n",
    "            z_mask[idx, :, :length] = True\n",
    "        \n",
    "        # make the attention_mask\n",
    "        attention_mask = x_mask.unsqueeze(3) * z_mask.unsqueeze(2) # (B, 1, T, 1) * (B, 1, 1, F) = (B, 1, T, F)\n",
    "            # 주의: Encoder의 attention_mask와는 다른 mask임.\n",
    "        \n",
    "        if not inference: # training\n",
    "            z, log_det = self.decoder(y, z_mask, reverse=False)\n",
    "            with torch.no_grad():\n",
    "                x_std_squared_root = torch.exp(-2 * x_log_std) # (B, 80, T)\n",
    "                logp1 = torch.sum(-0.5 * math.log(2 * math.pi) - x_log_std, [1]).unsqueeze(-1) # [(B, T, F)\n",
    "                logp2 = torch.matmul(x_std_squared_root.transpose(1, 2), -0.5 * (z ** 2)) # [(B, T, 80) * (B, 80, F) = (B, T, F)\n",
    "                logp3 = torch.matmul((x_mean * x_std_squared_root).transpose(1,2), z) # (B, T, 80) * (B, 80, F) = (B, T, F)\n",
    "                logp4 = torch.sum(-0.5 * (x_mean ** 2) * x_std_squared_root, [1]).unsqueeze(-1) # (B, T, F)\n",
    "                logp = logp1 + logp2 + logp3 + logp4 # (B, T, F)\n",
    "                \"\"\"\n",
    "                logp는 normal distribution N(x_mean, x_std)의 maximum log-likelihood이다.\n",
    "                sum(log(N(z;x_mean, x_std)))를 정규분포 식을 이용하여 분배법칙으로 풀어내면 위와 같은 식이 도출된다.\n",
    "                \"\"\"\n",
    "                attention_alignment = maximum_path(logp, attention_mask.squeeze(1)).detach() # alignment (B, T, F)\n",
    "             \n",
    "            z_mean = torch.matmul(attention_alignment.transpose(1, 2), x_mean.transpose(1, 2)) # (B, F, T) * (B, T, 80) -> (B, F, 80)\n",
    "            z_mean = z_mean.transpose(1, 2) # (B, 80, F)\n",
    "            z_log_std = torch.matmul(attention_alignment.transpose(1, 2), x_log_std.transpose(1, 2)) # (B, F, T) * (B, T, 80) -> (B, F, 80)\n",
    "            z_log_std = z_log_std.transpose(1, 2) # (B, 80, F)\n",
    "            log_d = torch.log(1e-8 + torch.sum(attention_alignment, -1)).unsqueeze(1) * x_mask # (B, 1, T) | alignment에서 형성된 duration의 log scale\n",
    "            return (z, z_mean, z_log_std, log_det, z_mask), (x_mean, x_log_std, x_mask), (attention_alignment, x_log_dur, log_d)\n",
    "            \n",
    "        else: # inference\n",
    "            # generate_path (make attention_alignment using ceil(x_dur))\n",
    "            attention_alignment = generate_path(ceil_dur.squeeze(1), attention_mask.squeeze(1)) # (B, T, F)\n",
    "            z_mean = torch.matmul(attention_alignment.transpose(1, 2), x_mean.transpose(1, 2)) # (B, F, T) * (B, T, 80) -> (B, F, 80)\n",
    "            z_mean = z_mean.transpose(1, 2) # (B, 80, F)\n",
    "            z_log_std = torch.matmul(attention_alignment.transpose(1, 2), x_log_std.transpose(1, 2)) # (B, F, T) * (B, T, 80) -> (B, F, 80)\n",
    "            z_log_std = z_log_std.transpose(1, 2) # (B, 80, F)\n",
    "            log_d = torch.log(1e-8 + torch.sum(attention_alignment, -1)).unsqueeze(1) * x_mask # (B, 1, T) | alignment에서 형성된 duration의 log scale\n",
    "            \n",
    "            z = (z_mean + torch.exp(z_log_std) * torch.randn_like(z_mean)) * z_mask # z(latent representation) 생성\n",
    "            y, log_det = self.decoder(z, z_mask, reverse=True) # mel-spectrogram 생성\n",
    "            return (y, z_mean, z_log_std, log_det, z_mask), (x_mean, x_log_std, x_mask), (attention_alignment, x_log_dur, log_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1dfc4",
   "metadata": {},
   "source": [
    "# 3. HiFi-GAN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962413c8",
   "metadata": {},
   "source": [
    "## 3.1. Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7ba140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2 model을 기준으로 한다.\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size):\n",
    "        \"\"\"\n",
    "        channels: \n",
    "        kernel_size: 3, 7, 11 중 하나\n",
    "        \"\"\"\n",
    "        super(ResBlock, self).__init__()\n",
    "        # padding = (kernel_size-1)*dilation//2 (\"same\")\n",
    "        self.convs1 = nn.ModuleList([\n",
    "            weight_norm(nn.Conv1d(channels, channels, kernel_size, stride=1, dilation=1,\n",
    "                                  padding=(kernel_size-1)*1//2)),\n",
    "            weight_norm(nn.Conv1d(channels, channels, kernel_size, stride=1, dilation=1,\n",
    "                                  padding=(kernel_size-1)*1//2))\n",
    "        ])\n",
    "        self.convs2 = nn.ModuleList([\n",
    "            weight_norm(nn.Conv1d(channels, channels, kernel_size, stride=1, dilation=3,\n",
    "                                  padding=(kernel_size-1)*3//2)),\n",
    "            weight_norm(nn.Conv1d(channels, channels, kernel_size, stride=1, dilation=1,\n",
    "                                  padding=(kernel_size-1)*1//2))\n",
    "        ])\n",
    "        self.convs3 = nn.ModuleList([\n",
    "            weight_norm(nn.Conv1d(channels, channels, kernel_size, stride=1, dilation=5,\n",
    "                                  padding=(kernel_size-1)*5//2)),\n",
    "            weight_norm(nn.Conv1d(channels, channels, kernel_size, stride=1, dilation=1,\n",
    "                                  padding=(kernel_size-1)*1//2))\n",
    "        ])\n",
    "        self.modules = [self.convs1, self.convs2, self.convs3]\n",
    "        \n",
    "        # 평균이 0, 표준편차가 0.01인 정규분포로 가중치 초기화\n",
    "        for module in self.modules:\n",
    "            for conv in module:\n",
    "                nn.init.normal_(conv.weight, mean=0.0, std=0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, channels, F) # mel-spectrogram으로부터 얻어진 input features\n",
    "        =====outputs=====\n",
    "        x: (B, channels, F) # mel-spectrogram으로부터 얻어진 output features\n",
    "        \"\"\"\n",
    "        for module in self.modules:\n",
    "            for conv in module:\n",
    "                y = F.leaky_relu(x, 0.1)\n",
    "                y = conv(y)\n",
    "            x = x + y\n",
    "        return x\n",
    "\n",
    "class MRF(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        channels: \n",
    "        \"\"\"\n",
    "        super(MRF, self).__init__()\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResBlock(channels, kernel_size=3),\n",
    "            ResBlock(channels, kernel_size=7),\n",
    "            ResBlock(channels, kernel_size=11),\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, channels, F)\n",
    "        =====outputs=====\n",
    "        x: (B, channels, F)\n",
    "        \"\"\"\n",
    "        skip_list = []\n",
    "        for res_block in self.res_blocks:\n",
    "            skip_x = res_block(x)\n",
    "            skip_list.append(skip_x)\n",
    "        x = sum(skip_list) / len(self.res_blocks)\n",
    "        return x\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.pre_conv = weight_norm(nn.Conv1d(80, 128, kernel_size=7, stride=1, dilation=1,\n",
    "                                              padding=(7-1)//2)) # (B, 80, F) -> (B, 128, F)\n",
    "        nn.init.normal_(self.pre_conv.weight, mean=0.0, std=0.01) # 논문 저자 구현에는 없음.\n",
    "        \n",
    "        self.up_convs = nn.ModuleList()\n",
    "        self.mrfs = nn.ModuleList()\n",
    "        ku = [16, 16, 4, 4]\n",
    "        for i in range(4):\n",
    "            # ku//2 배 upsampling\n",
    "            channels = 128//(2**(i+1))\n",
    "            up_conv = weight_norm(nn.ConvTranspose1d(128//(2**i), channels, kernel_size=ku[i], stride=ku[i]//2,\n",
    "                                                     padding=(ku[i]-ku[i]//2)//2))\n",
    "                # (B, 128, F) -(1)-> (B, 64, F*8) -(2)-> (B, 32, F*8*8) -(3)-> (B, 16, F*8*8*2) -(4)-> (B, 8, F*8*8*2*2)\n",
    "            nn.init.normal_(up_conv.weight, mean=0.0, std=0.01)\n",
    "            self.up_convs.append(up_conv)\n",
    "            \n",
    "            # MRF\n",
    "            mrf = MRF(channels) # (B, channels, F) -> (B, channels, F)\n",
    "            self.mrfs.append(mrf)\n",
    "            \n",
    "        self.post_conv = weight_norm(nn.Conv1d(8, 1, kernel_size=7, stride=1, dilation=1,\n",
    "                                               padding=(7-1)//2)) # (B, 8, F*256) -> (B, 1, F*256)\n",
    "        nn.init.normal_(self.post_conv.weight, mean=0.0, std=0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 80, F) # mel_spectrogram\n",
    "        =====outputs=====\n",
    "        x: (B, 1, F*256) # waveform\n",
    "        \"\"\"\n",
    "        x = self.pre_conv(x) # (B, 80, F) -> (B, 128, F)\n",
    "        for i in range(4):\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            x = self.up_convs[i](x)\n",
    "            x = self.mrfs[i](x)\n",
    "            # final: (B, 128, F) -> (B, 8, F*256)\n",
    "        x = F.leaky_relu(x, 0.1)\n",
    "        x = self.post_conv(x) # (B, 8, F*256) -> (B, 1, F*256)\n",
    "        x = torch.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b61a4c",
   "metadata": {},
   "source": [
    "## 3.2. Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca588f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubPD(nn.Module):\n",
    "    def __init__(self, period):\n",
    "        \"\"\"\n",
    "        period: 2, 3, 5, 7, 11 중 하나\n",
    "        \"\"\"\n",
    "        super(SubPD, self).__init__()\n",
    "        self.period = period\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        channels = 1\n",
    "        for i in range(1, 5): # 논문 저자의 변형 구현 대신 논문대로 구현함.\n",
    "            conv = weight_norm(nn.Conv2d(channels, 2**(5+i), kernel_size=(5, 1), stride=(3, 1), dilation=1, padding=0))\n",
    "            self.convs.append(conv)\n",
    "            channels = 2**(5+i)\n",
    "            # (B, 1, [T/p]+1, p) -(1)-> (B, 64, ?, p) -(2)-> (B, 128, ?, p) -(3)-> (B, 256, ?, p) -(4)-> (B, 512, ?, p)\n",
    "        last_conv = weight_norm(nn.Conv2d(channels, 1024, kernel_size=(5, 1), stride=(1, 1), dilation=1,\n",
    "                                          padding=(2, 0))) # (B, 512, ?, p) -> (B, 1024, ?, p)\n",
    "        self.convs.append(last_conv)\n",
    "        \n",
    "        self.post_conv = weight_norm(nn.Conv2d(1024, 1, kernel_size=(3, 1), stride=(1, 1), dilation=1,\n",
    "                                               padding=(1, 0))) # (B, 1024, ?, p) -> (B, 1, ?, p)\n",
    "        \n",
    "    def forward(self, waveform):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        waveform: (B, 1, T)\n",
    "        =====outputs=====\n",
    "        x: (B, ?) # flatten된 real/fake 벡터 (0~1(?))\n",
    "        features: feature를 모두 모아놓은 list (Feature Matching Loss를 계산하기 위함.)\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        \n",
    "        B, _, T = waveform.size()\n",
    "        P = self.period\n",
    "        # padding\n",
    "        if T % P != 0:\n",
    "            padding = P - (T % P)\n",
    "            waveform = F.pad(waveform, (0, padding), \"reflect\") # 앞쪽에 0, 뒤쪽에 padding만큼 패딩, reflect는 마치 거울에 반사되듯이 패딩함.\n",
    "                # ex) [1, 2, 3, 4, 5]를 앞쪽에 2, 뒤쪽에 3만큼 reflect 모드로 padding -> [3, 2, 1, 2, 3, 4, 5, 4, 3, 2]\n",
    "            T += padding\n",
    "        # reshape\n",
    "        x = waveform.view(B, 1, T//P, P) # (B, 1, [T/P]+1, P)\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            features.append(x)\n",
    "        x = self.post_conv(x)\n",
    "        features.append(x)\n",
    "        x = torch.flatten(x, 1, -1) # index 1번째 차원부터 마지막 차원까지 flatten | (B, ?)\n",
    "        return x, features\n",
    "\n",
    "class MPD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MPD, self).__init__()\n",
    "        self.sub_pds = nn.ModuleList([\n",
    "            SubPD(2), SubPD(3), SubPD(5), SubPD(7), SubPD(11), \n",
    "        ]) # (B, 1, T) -> (B, ?), features list\n",
    "        \n",
    "    def forward(self, real_waveform, gen_waveform):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        real_waveform: (B, 1, T) # 실제 음성\n",
    "        gen_waveform: (B, 1, T) # 생성 음성\n",
    "        =====outputs=====\n",
    "        real_outputs: (B, ?) list (len=5) # 실제 음성에 대한 SubPD outputs list\n",
    "        gen_outputs: (B, ?) list # 생성 음성에 대한 SubPD outputs list\n",
    "        real_features: features list # 실제 음성에 대한 SubPD features list\n",
    "        gen_features: features list # 생성 음성에 대한 SubPD features list\n",
    "        \"\"\"\n",
    "        real_outputs, gen_outputs, real_features, gen_features = [], [], [], []\n",
    "        for sub_pd in self.sub_pds:\n",
    "            real_output, real_feature = sub_pd(real_waveform)\n",
    "            gen_output, gen_feature = sub_pd(gen_waveform)\n",
    "            real_outputs.append(real_output)\n",
    "            gen_outputs.append(gen_output)\n",
    "            real_features.append(real_feature)\n",
    "            gen_features.append(gen_feature)\n",
    "        return real_outputs, gen_outputs, real_features, gen_features\n",
    "    \n",
    "class SubSD(nn.Module):\n",
    "    def __init__(self, first=False):\n",
    "        \"\"\"\n",
    "        first: boolean (first가 True이면 spectral normalization을 적용한다.)\n",
    "        \"\"\"\n",
    "        super(SubSD, self).__init__()\n",
    "        norm = spectral_norm if first else weight_norm # first가 True이면 spectral_norm, 그렇지 않으면 weight_norm\n",
    "        self.convs = nn.ModuleList([ # Mel-GAN 논문에 맞게 구현\n",
    "            norm(nn.Conv1d(1, 16, kernel_size=15, stride=1, padding=(15-1)//2)), # (B, 1, T) -> (B, 16, T)\n",
    "            norm(nn.Conv1d(16, 64, kernel_size=41, stride=4, groups=4, padding=(41-1)//2)), # (B, 16, T) -> (B, 64, T/4(?))\n",
    "            norm(nn.Conv1d(64, 256, kernel_size=41, stride=4, groups=16, padding=(41-1)//2)), # (B, 64, T/4(?)) -> (B, 256, T/16(?))\n",
    "            norm(nn.Conv1d(256, 1024, kernel_size=41, stride=4, groups=64, padding=(41-1)//2)), # (B, 256, T/16(?)) -> (B, 1024, T/64(?))\n",
    "            norm(nn.Conv1d(1024, 1024, kernel_size=41, stride=4, groups=256, padding=(41-1)//2)), # (B, 1024, T/64(?)) -> (B, 1024, T/256(?))\n",
    "            norm(nn.Conv1d(1024, 1024, kernel_size=5, stride=1, padding=(5-1)//2)) # (B, 1024, T/256(?)) -> (B, 1024, T/256(?))\n",
    "        ])\n",
    "        self.post_conv = norm(nn.Conv1d(1024, 1, kernel_size=3, stride=1, padding=(3-1)//2)) # (B, 1024, ?) -> (B, 1, ?)\n",
    "        \n",
    "    def forward(self, waveform):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        waveform: (B, 1, T)\n",
    "        =====outputs=====\n",
    "        x: (B, ?) # flatten된 real/fake 벡터 (0~1(?))\n",
    "        features: feature를 모두 모아놓은 list (Feature Matching Loss를 계산하기 위함.)\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        x = waveform\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            features.append(x)\n",
    "        x = self.post_conv(x) # (B, 1, ?)\n",
    "        features.append(x)\n",
    "        x = x.squeeze(1) # (B, ?)\n",
    "        return x, features\n",
    "    \n",
    "class MSD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MSD, self).__init__()\n",
    "        self.sub_sds = nn.ModuleList([\n",
    "            SubSD(first=True), SubSD(), SubSD() \n",
    "        ]) # (B, 1, T) -> (B, ?), features list\n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=4, stride=2, padding=2) # x2 down sampling\n",
    "        \n",
    "    def forward(self, real_waveform, gen_waveform):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        real_waveform: (B, 1, T) # 실제 음성\n",
    "        gen_waveform: (B, 1, T) # 생성 음성\n",
    "        =====outputs=====\n",
    "        real_outputs: (B, ?) list (len=3) # 실제 음성에 대한 SubSD outputs list\n",
    "        gen_outputs: (B, ?) list # 생성 음성에 대한 SubSD outputs list\n",
    "        real_features: features list # 실제 음성에 대한 SubSD features list\n",
    "        gen_features: features list # 생성 음성에 대한 SubSD features list\n",
    "        \"\"\"\n",
    "        real_outputs, gen_outputs, real_features, gen_features = [], [], [], []\n",
    "        for idx, sub_sd in enumerate(self.sub_sds):\n",
    "            if idx != 0:\n",
    "                real_waveform = self.avgpool(real_waveform)\n",
    "                gen_waveform = self.avgpool(gen_waveform)\n",
    "            real_output, real_feature = sub_sd(real_waveform)\n",
    "            gen_output, gen_feature = sub_sd(gen_waveform)\n",
    "            real_outputs.append(real_output)\n",
    "            gen_outputs.append(gen_output)\n",
    "            real_features.append(real_feature)\n",
    "            gen_features.append(gen_feature)\n",
    "        return real_outputs, gen_outputs, real_features, gen_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782a64f0",
   "metadata": {},
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bad54c",
   "metadata": {},
   "source": [
    "## 4.0. Plot mel-spectrogram & Plot Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d51a38",
   "metadata": {},
   "source": [
    "- 각각 HiFi-GAN과 Tacotron2에서 구현을 가져옴. Plot Alignment는 모델에 맞게 변형함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c8ae053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mel(mel, step=None, loss=None):\n",
    "    \"\"\"\n",
    "    =====inputs=====\n",
    "    mel: (80, F) # 입력 시 .detach().cpu().numpy() 를 적용하는 것을 잊지 말 것!\n",
    "    \"\"\"\n",
    "    plt.rcParams['axes.unicode_minus'] = False # 마이너스 부호 오류에 대한 해결\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    im = ax.imshow(mel, origin=\"lower\", aspect=\"auto\")\n",
    "    plt.title(f\"Step: {step} | Loss: {loss}\")\n",
    "    plt.xlabel(\"time\")\n",
    "    plt.ylabel(\"frequency_bin\")\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    plt.close()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c485e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.use('Agg')\n",
    "\n",
    "font_name = fm.FontProperties(fname=\"C:/Users/Poco/Jupyter/PaperReview/dataset/malgun.ttf\").get_name()\n",
    "matplotlib.rc('font', family=font_name, size=14)\n",
    "\n",
    "def plot_alignment(alignment, text, step, loss):\n",
    "    text = text.rstrip('_').rstrip('~')\n",
    "    alignment = alignment[:len(text)]\n",
    "    \n",
    "    # 하나의 그림(fig) 객체와 하나의 축(ax) 객체를 생성\n",
    "    fig, ax = plt.subplots(figsize=(len(text)/3, 5))\n",
    "    # 생성한 축(ax) 객체에 이미지를 출력\n",
    "    im = ax.imshow(np.transpose(alignment), aspect='auto', origin='lower')\n",
    "    \n",
    "    plt.title(f\"step: {step}, loss: {loss:.5f}\", loc=\"center\", pad=10)\n",
    "    plt.xlabel('Encoder timestep')\n",
    "    plt.ylabel('Decoder timestep')\n",
    "    # 공백 문자 ' '를 빈 문자열 ''로 변환\n",
    "    text = [x if x != ' ' else '' for x in list(text)]\n",
    "    # x축의 눈금과 레이블을 설정\n",
    "    plt.xticks(range(len(text)), text)\n",
    "    \n",
    "    # 그래프의 레이아웃을 조정\n",
    "    plt.tight_layout()\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    fig.canvas.draw()\n",
    "    plt.close()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5304a28",
   "metadata": {},
   "source": [
    "## 4.1. Glow-TTS Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3de069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLE_Loss(z, z_mean, z_log_std, log_det, z_mask): # Maximum Likelihood Estimation\n",
    "    \"\"\"\n",
    "    =====input=====\n",
    "    (1st return tuple of the model)\n",
    "    =====output=====\n",
    "    Loss: (1) | Negative Loglikelihood\n",
    "    \"\"\"\n",
    "    Loss = torch.sum(z_log_std) + 0.5 * torch.sum(torch.exp(-2 * z_log_std) * ((z-z_mean)**2)) # (1)\n",
    "    Loss = Loss - torch.sum(log_det) # (2)\n",
    "    Loss = Loss / torch.sum(torch.ones_like(z) * z_mask) # (3) \n",
    "    Loss = Loss + 0.5 * math.log(2 * math.pi) # (4)\n",
    "    \"\"\"\n",
    "    Negative Loglikelihood\n",
    "    (1) + (3)*(4) + (2) : 논문에서 제시된 Loglikelihood에 음을 취한 값 (Negative Loglikelihood)\n",
    "    여기에서 전체에 (3)으로 나눠주었다고 생각하면 된다. (Averaging across B, C=80, F)\n",
    "    \"\"\"\n",
    "    return Loss\n",
    "\n",
    "def Duration_Loss(x_log_dur, log_d, text_len):\n",
    "    \"\"\"\n",
    "    =====inputs=====\n",
    "    x_log_dur: (B, 1, T) | 예측된 duration () (from 3rd returned tuple)\n",
    "    log_d: (B, 1, T) | Alignment로부터 추출한 duration (from 3rd returned tuple)\n",
    "    text_len: (B) | text의 길이 (from dataset)\n",
    "    =====outputs=====\n",
    "    Loss: (1) | Duration Loss\n",
    "    \"\"\"\n",
    "    Loss = torch.sum((x_log_dur - log_d)**2) / torch.sum(text_len)\n",
    "    # MSE | torch 구현을 사용하지 않은 이유는 text_len이 모두 다르기 때문이다.\n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b37a0",
   "metadata": {},
   "source": [
    "## 4.2. Glow-TTS Adam Optimizer + Noam LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47ad0e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam():\n",
    "    def __init__(self, params, dim_model=192, warmup_steps=4000, lr=1e0, betas=(0.9, 0.98)):\n",
    "        self.params = params\n",
    "        self.dim_model = dim_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "\n",
    "        self.step_num = 1\n",
    "        self.current_lr = lr * self._get_lr_scale()\n",
    "\n",
    "        self._optim = torch.optim.Adam(params, lr=self.current_lr, betas=betas)\n",
    "    \n",
    "    def _get_lr_scale(self):\n",
    "        return np.power(self.dim_model, -0.5) * np.min([np.power(self.step_num, -0.5),\n",
    "                                                        self.step_num * np.power(self.warmup_steps, -1.5)])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        self.step_num += 1\n",
    "        self.current_lr = self.lr * self._get_lr_scale() # NoamLR\n",
    "        for param_group in self._optim.param_groups:\n",
    "            param_group['lr'] = self.current_lr\n",
    "    \n",
    "    def get_lr(self):\n",
    "        return self.current_lr\n",
    "    \n",
    "    def step(self):\n",
    "        self._optim.step()\n",
    "        self._update_learning_rate()\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self._optim.zero_grad()\n",
    "    \n",
    "    def load_state_dict(self, d):\n",
    "        self._optim.load_state_dict(d)\n",
    "        \n",
    "    def state_dict(self):\n",
    "        return self._optim.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9445b575",
   "metadata": {},
   "source": [
    "## 4.3. Glow-TTS Main Training Function using HiFi-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4402221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Glow_TTS_train(model_name, check_step, hifi_model_name=None, hifi_check_step=None):\n",
    "    \"\"\"\n",
    "    (NEW) check_step = None\n",
    "    (LOAD) check_step: 불러오고자 하는 모델의 step\n",
    "    \n",
    "    만약 Glow-TTS로부터 생성된 Mel-spectrogram을 pretrained된 HiFi-GAN으로 학습시키길 원한다면\n",
    "    HiFi-GAN 모델명과 check_step을 적으라.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Define Models & Optimizer\n",
    "    model = GlowTTS().to(device)\n",
    "    \n",
    "    optim = Adam(model.parameters())\n",
    "\n",
    "    # Load Models & Optimizer\n",
    "    epoch = 1\n",
    "    step = 1\n",
    "    if check_step is not None:\n",
    "        os.makedirs('ckpt/' + model_name, exist_ok=True)\n",
    "        check_point = './ckpt/' + model_name + \"/ckpt-\" + str(check_step) + \".pt\"\n",
    "        ckpt = torch.load(check_point)\n",
    "        model.load_state_dict(ckpt['model'])\n",
    "        optim.load_state_dict(ckpt['optim'])\n",
    "        epoch = ckpt['epoch']\n",
    "        step = ckpt['step']\n",
    "        print(f'Load Glow-TTS model: {model_name} | Step {check_step}')\n",
    "        \n",
    "    if hifi_check_step is not None:\n",
    "        gen_model = Generator().to(device) # HiFi-GAN\n",
    "        check_point = './hifi_ckpt/' + hifi_model_name + \"/ckpt-\" + str(hifi_check_step) + \".pt\"\n",
    "        ckpt = torch.load(check_point)\n",
    "        gen_model.load_state_dict(ckpt['gen_model'])\n",
    "        print(f'Load HiFi-GAN model: {hifi_model_name} | Step {hifi_check_step}')\n",
    "    \n",
    "    # cuDNN의 벤치마크 모드를 활성화. 합성곱과 풀링 연산 등을 최적화.\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Training Mode Activation\n",
    "    model.train()\n",
    "    \n",
    "    # tensorboard\n",
    "    os.makedirs('ckpt/' + model_name + '/logs', exist_ok=True)\n",
    "    sw = SummaryWriter('./ckpt/' + model_name + '/logs')\n",
    "    \n",
    "    # Main Training\n",
    "    start = time.time() # start time 초기화\n",
    "    while True:\n",
    "        print(f\"|| Epoch: {epoch} ||\")\n",
    "        for _, batch in enumerate(train_dataloader):\n",
    "            text = batch[0].to(device)\n",
    "            text_len = torch.tensor(batch[1], dtype=torch.int32)\n",
    "            mel = batch[2].to(device).transpose(1, 2)\n",
    "            mel_len = torch.tensor(batch[3], dtype=torch.int32)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            (z, z_mean, z_log_std, log_det, z_mask), (x_mean, x_log_std, x_mask), (attention_alignment, x_log_dur, log_d) = \\\n",
    "                model(text, text_len, mel, mel_len)\n",
    "            \n",
    "            mle_loss = MLE_Loss(z, z_mean, z_log_std, log_det, z_mask)\n",
    "            duration_loss = Duration_Loss(x_log_dur, log_d, text_len)\n",
    "            loss = mle_loss + duration_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optim.step()\n",
    "            step += 1\n",
    "            \n",
    "            # Logging\n",
    "            if step % logging_step == 0:\n",
    "                print(f'| step: {step} | loss: {loss:2.3f} | mle_loss: {mle_loss:2.3f} | dur_loss: {duration_loss:2.3f} | {time.time()-start:.3f} sec / {logging_step} steps |')\n",
    "                # Tensorboard에 기록\n",
    "                sw.add_scalar(\"training/loss\", loss, step)\n",
    "                sw.add_scalar(\"training/mle_loss\", mle_loss, step)\n",
    "                sw.add_scalar(\"training/duration_loss\", duration_loss, step)\n",
    "                sw.add_scalar(\"training/grad_norm\", grad_norm, step)\n",
    "                sw.add_scalar(\"training/learning_rate\", optim.get_lr(), step)\n",
    "                start = time.time() # start time 초기화\n",
    "            \n",
    "            # Validation\n",
    "            if step % validation_step == 0:\n",
    "                model.eval()\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                loss_sum = 0\n",
    "                mle_loss_sum = 0\n",
    "                duration_loss_sum = 0\n",
    "                with torch.no_grad():\n",
    "                    for i, batch in enumerate(val_dataloader):\n",
    "                        text = batch[0].to(device)\n",
    "                        text_len = torch.tensor(batch[1], dtype=torch.int32)\n",
    "                        mel = batch[2].to(device).transpose(1, 2)\n",
    "                        mel_len = torch.tensor(batch[3], dtype=torch.int32)\n",
    "\n",
    "                        optim.zero_grad()\n",
    "                        (z, z_mean, z_log_std, log_det, z_mask), (x_mean, x_log_std, x_mask), (attention_alignment, x_log_dur, log_d) = \\\n",
    "                            model(text, text_len, mel, mel_len)\n",
    "\n",
    "                        mle_loss = MLE_Loss(z, z_mean, z_log_std, log_det, z_mask)\n",
    "                        duration_loss = Duration_Loss(x_log_dur, log_d, text_len)\n",
    "                        loss = mle_loss + duration_loss\n",
    "                        \n",
    "                        loss_sum += loss\n",
    "                        mle_loss_sum += mle_loss\n",
    "                        duration_loss_sum += duration_loss\n",
    "                        \n",
    "                        # inference\n",
    "                        (y, z_mean, z_log_std, log_det, z_mask), (x_mean, x_log_std, x_mask), (attention_alignment, x_log_dur, log_d) = \\\n",
    "                            model(text, text_len, inference=True)\n",
    "                        \n",
    "                        if (i >= 5 and i <= 9): # 그림은 5개 저장\n",
    "                            sw.add_figure(f'validation/gen_mel_{i})',\n",
    "                                          plot_mel(y[0].detach().cpu().numpy(), step, loss),\n",
    "                                          step)\n",
    "                            sw.add_figure(f'validation/org_mel_{i})',\n",
    "                                          plot_mel(mel[0].detach().cpu().numpy(), step, loss),\n",
    "                                          step)\n",
    "                            input_seq = sequence_to_text(text[0].cpu().numpy())\n",
    "                            input_seq = input_seq[:text_len[0]]\n",
    "                            sw.add_figure(f'validation/gen_align_{i})',\n",
    "                                          plot_alignment(attention_alignment[0].detach().cpu().numpy(), input_seq, step, loss),\n",
    "                                          step)\n",
    "                            if hifi_check_step is not None:\n",
    "                                gen_wav = gen_model(y[0]) # (1, L)\n",
    "                                sw.add_audio(f'generated/gen_wav_{i}', gen_wav[0], step, sample_rate)\n",
    "                                \n",
    "                        \n",
    "                    loss_avg = loss_sum / (i+1)\n",
    "                    mle_loss_avg = mle_loss_sum / (i+1)\n",
    "                    duration_loss_avg = duration_loss_sum / (i+1)\n",
    "                    \n",
    "                    sw.add_scalar(\"validation/loss\", loss_avg, step)\n",
    "                    sw.add_scalar(\"validation/mle_loss\", mle_loss_avg, step)\n",
    "                    sw.add_scalar(\"validation/duration_loss\", duration_loss_avg, step)\n",
    "                    print(f\"[Validation] loss: {loss_avg:2.3f} | mle_loss: {mle_loss_avg:2.3f} | dur_loss: {duration_loss_avg:2.3f} | {time.time()-start:.3f} sec\")\n",
    "                \n",
    "                model.train()\n",
    "                start = time.time() # start time 초기화\n",
    "                        \n",
    "            # Checkpoint\n",
    "            if step % checkpoint_step == 0:\n",
    "                save_dir = './ckpt/' + model_name\n",
    "                torch.save({\n",
    "                    'model': model.state_dict(),\n",
    "                    'optim': optim.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'step': step\n",
    "                }, os.path.join(save_dir, 'ckpt-{}.pt'.format(step)))\n",
    "                \n",
    "                start = time.time() # start time 초기화\n",
    "                \n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a9c4e9",
   "metadata": {},
   "source": [
    "## 4.4. HiFi-GAN Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cf781d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAN_Loss_Generator(gen_outputs):\n",
    "    \"\"\"\n",
    "    gen_outputs: (B, ?) list # MPD(len=5) 또는 MSD(len=3)의 출력\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    for DG in gen_outputs:\n",
    "        loss += torch.mean((DG-1)**2)\n",
    "    return loss\n",
    "\n",
    "def GAN_Loss_Discriminator(real_outputs, gen_outputs):\n",
    "    \"\"\"\n",
    "    real_outputs: (B, ?) list # MPD(len=5) 또는 MSD(len=3)의 출력\n",
    "    gen_outputs: (B, ?) list # MPD(len=5) 또는 MSD(len=3)의 출력\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    for D, DG in zip(real_outputs, gen_outputs):\n",
    "        loss += torch.mean((D-1)**2 + DG**2)\n",
    "    return loss\n",
    "\n",
    "def Mel_Spectrogram_Loss(real_mel, gen_mel):\n",
    "    \"\"\"\n",
    "    real_mel: (B, F, 80) # Dataloader로부터 가져온 mel-spectrogram\n",
    "    gen_mel: (B, F, 80) # Generator가 생성한 waveform의 mel-spectrogram\n",
    "    \"\"\"\n",
    "    loss = F.l1_loss(real_mel, gen_mel)\n",
    "    return 45*loss\n",
    "\n",
    "def Feature_Matching_Loss(real_features, gen_features):\n",
    "    \"\"\"\n",
    "    real_features: (?, ..., ?) list of list # MPD(len=[5, 6]) 또는 MSD(len=[3, 7])의 출력\n",
    "    gen_features: (?, ..., ?) list of list # MPD(len=[5, 6]) 또는 MSD(len=[3, 7])의 출력\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    for Ds, DGs in zip(real_features, gen_features):\n",
    "        for D, DG in zip(Ds, DGs):\n",
    "            loss += torch.mean(torch.abs(D - DG))\n",
    "    return 2*loss\n",
    "\n",
    "def Final_Loss_Generator(mpd_gen_outputs, mpd_real_features, mpd_gen_features,\n",
    "                         msd_gen_outputs, msd_real_features, msd_gen_features,\n",
    "                         real_mel, gen_mel):\n",
    "    \"\"\"\n",
    "    =====inputs=====\n",
    "    [:3]: MPD outputs 뒤쪽 3개\n",
    "    [3:6]: MSD outputs 뒤쪽 3개\n",
    "    [7:8]: real_mel and gen_mel\n",
    "    =====outputs=====\n",
    "    Generator_Loss\n",
    "    Mel_Loss\n",
    "    \"\"\"\n",
    "    Gen_Adv1 = GAN_Loss_Generator(mpd_gen_outputs)\n",
    "    Gen_Adv2 = GAN_Loss_Generator(msd_gen_outputs)\n",
    "    Adv = Gen_Adv1 + Gen_Adv2\n",
    "    FM1 = Feature_Matching_Loss(mpd_real_features, mpd_gen_features)\n",
    "    FM2 = Feature_Matching_Loss(msd_real_features, msd_gen_features)\n",
    "    FM = FM1 + FM2\n",
    "    Mel_Loss = Mel_Spectrogram_Loss(real_mel, gen_mel)\n",
    "    Generator_Loss = Adv + FM + Mel_Loss\n",
    "    \n",
    "    return Generator_Loss, Mel_Loss\n",
    "\n",
    "def Final_Loss_Discriminator(mpd_real_outputs, mpd_gen_outputs,\n",
    "                             msd_real_outputs, msd_gen_outputs):\n",
    "    \"\"\"\n",
    "    =====inputs=====\n",
    "    [:2]: MPD outputs 앞쪽 2개\n",
    "    [2:4]: MSD outputs 앞쪽 2개\n",
    "    =====outputs=====\n",
    "    Discriminator_Loss\n",
    "    \"\"\"\n",
    "    Disc_Adv1 = GAN_Loss_Discriminator(mpd_real_outputs, mpd_gen_outputs)\n",
    "    Disc_Adv2 = GAN_Loss_Discriminator(msd_real_outputs, msd_gen_outputs)\n",
    "    Discriminator_Loss = Disc_Adv1 + Disc_Adv2\n",
    "    \n",
    "    return Discriminator_Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ec912",
   "metadata": {},
   "source": [
    "## 4.5. HiFi-GAN Mel-spectrogram Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff6d35c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HiFi-GAN 저자 구현\n",
    "def mel_spectrogram(y, n_fft=1024, num_mels=80, sampling_rate=22050, hop_size=256, win_size=1024, fmin=0, fmax=8000, center=False):\n",
    "    \"\"\"\n",
    "    if torch.min(y) < -1.:\n",
    "        print('min value is ', torch.min(y))\n",
    "    if torch.max(y) > 1.:\n",
    "        print('max value is ', torch.max(y))\n",
    "    \"\"\"\n",
    "\n",
    "    mel = librosa.filters.mel(sampling_rate, n_fft, num_mels, fmin, fmax)\n",
    "    \n",
    "    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n",
    "    y = y.squeeze(1)\n",
    "\n",
    "    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=torch.hann_window(win_size).to(y.device),\n",
    "                      center=center, pad_mode='reflect', normalized=False, onesided=True)\n",
    "\n",
    "    spec = torch.sqrt(spec.pow(2).sum(-1)+(1e-9))\n",
    "\n",
    "    spec = torch.matmul(torch.from_numpy(mel).float().to(y.device), spec)\n",
    "    spec = torch.log(torch.clamp(spec, min=1e-5) * 1)\n",
    "\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcacf91",
   "metadata": {},
   "source": [
    "## 4.6. HiFi-GAN Main Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fd3ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HiFi_GAN_train(model_name, check_step):\n",
    "    \"\"\"\n",
    "    (NEW) check_step = None\n",
    "    (LOAD) check_step: 불러오고자 하는 모델의 step\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Define Models & Optimizer\n",
    "    gen_model = Generator().to(device)\n",
    "    mpd_model = MPD().to(device)\n",
    "    msd_model = MSD().to(device)\n",
    "    \n",
    "    gen_optim = torch.optim.AdamW(gen_model.parameters(),\n",
    "                                  lr=0.0002, betas=(0.8, 0.99))\n",
    "    disc_optim = torch.optim.AdamW(itertools.chain(mpd_model.parameters(),\n",
    "                                                   msd_model.parameters()),\n",
    "                                   lr=0.0002, betas=(0.8, 0.99))\n",
    "    # 오류에 따른 수정: initial_lr 지정\n",
    "    for param_group in gen_optim.param_groups:\n",
    "        param_group['initial_lr'] = 0.0002\n",
    "    \n",
    "    for param_group in disc_optim.param_groups:\n",
    "        param_group['initial_lr'] = 0.0002\n",
    "    \n",
    "    # Load Models & Optimizer\n",
    "    epoch = 1\n",
    "    step = 1\n",
    "    if check_step is not None:\n",
    "        os.makedirs('hifi_ckpt/' + model_name, exist_ok=True)\n",
    "        check_point = './hifi_ckpt/' + model_name + \"/ckpt-\" + str(check_step) + \".pt\"\n",
    "        ckpt = torch.load(check_point)\n",
    "        gen_model.load_state_dict(ckpt['gen_model'])\n",
    "        mpd_model.load_state_dict(ckpt['mpd_model'])\n",
    "        msd_model.load_state_dict(ckpt['msd_model'])\n",
    "        gen_optim.load_state_dict(ckpt['gen_optim'])\n",
    "        disc_optim.load_state_dict(ckpt['disc_optim'])\n",
    "        epoch = ckpt['epoch']\n",
    "        step = ckpt['step']\n",
    "        print(f'Load {model_name} | Step {check_step}')\n",
    "    \n",
    "    # cuDNN의 벤치마크 모드를 활성화. 합성곱과 풀링 연산 등을 최적화.\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Define Scheduler\n",
    "    gen_scheduler = torch.optim.lr_scheduler.ExponentialLR(gen_optim, gamma=0.999,\n",
    "                                                           last_epoch=epoch)\n",
    "    disc_scheduler = torch.optim.lr_scheduler.ExponentialLR(disc_optim, gamma=0.999,\n",
    "                                                            last_epoch=epoch)\n",
    "    \n",
    "    # Training Mode Activation\n",
    "    gen_model.train()\n",
    "    mpd_model.train()\n",
    "    msd_model.train()\n",
    "    \n",
    "    # tensorboard\n",
    "    os.makedirs('hifi_ckpt/' + model_name + '/logs', exist_ok=True)\n",
    "    sw = SummaryWriter('./hifi_ckpt/' + model_name + '/logs')\n",
    "    \n",
    "    # Main Training\n",
    "    \"\"\"\n",
    "    mel과 mel_loss를 구분하여 논문 저자 구현을 따라감.\n",
    "    \"\"\"\n",
    "    start = time.time() # start time 초기화\n",
    "    while True:\n",
    "        print(f\"|| Epoch: {epoch} ||\")\n",
    "        for _, batch in enumerate(train_dataloader):\n",
    "            _, _, mel, _, wav, _ = batch\n",
    "            real_mel = mel.to(device) # (B, Max_F, 80)\n",
    "            real_wav = wav.to(device) # (B, Max_L)\n",
    "            real_mel = real_mel.transpose(1, 2) # (B, 80, Max_F) # fmax=8000\n",
    "            real_wav = real_wav.unsqueeze(1) # (B, 1, Max_L)\n",
    "            _, _, Max_L = real_wav.size()\n",
    "            \n",
    "            real_mel_for_loss = mel_spectrogram(real_wav.squeeze(1), fmax=11025)\n",
    "            \n",
    "            # Generator\n",
    "            gen_wav = gen_model(real_mel)\n",
    "            _, _, Gen_L = gen_wav.size()\n",
    "            if Max_L < Gen_L:\n",
    "                gen_wav = gen_wav[:, :, :Max_L]\n",
    "            else:\n",
    "                real_wav = real_wav[:, :, :Gen_L]\n",
    "            \n",
    "            gen_mel = mel_spectrogram(gen_wav.squeeze(1), fmax=11025)\n",
    "            \"\"\"\n",
    "            Nyquist Frequency에 의하면 sampling rate의 절반에 해당하는 주파수까지 분석이 가능하다.\n",
    "            현재 sampling rate이 22050이므로, fmax를 그의 절반인 11025로 둔다.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Discriminator Loss\n",
    "            \n",
    "            # MPD\n",
    "            mpd_real_outputs, mpd_gen_outputs, mpd_real_features, mpd_gen_features = mpd_model(real_wav, gen_wav.detach())\n",
    "            # MSD\n",
    "            msd_real_outputs, msd_gen_outputs, msd_real_features, msd_gen_features = msd_model(real_wav, gen_wav.detach())\n",
    "                # Discriminator에서는 gen_wav에 대한 gradient를 계산하지 않음을 명심하라.\n",
    "\n",
    "            disc_optim.zero_grad()\n",
    "            disc_loss = Final_Loss_Discriminator(mpd_real_outputs, mpd_gen_outputs,\n",
    "                                                 msd_real_outputs, msd_gen_outputs)\n",
    "            disc_loss.backward()\n",
    "            disc_optim.step()\n",
    "            \n",
    "            # Generator Loss\n",
    "            \n",
    "            # MPD\n",
    "            mpd_real_outputs, mpd_gen_outputs, mpd_real_features, mpd_gen_features = mpd_model(real_wav, gen_wav)\n",
    "            # MSD\n",
    "            msd_real_outputs, msd_gen_outputs, msd_real_features, msd_gen_features = msd_model(real_wav, gen_wav)\n",
    "            \n",
    "            gen_optim.zero_grad() # 이전 step의 gradient 초기화\n",
    "            gen_loss, mel_loss = Final_Loss_Generator(mpd_gen_outputs, mpd_real_features, mpd_gen_features,\n",
    "                                            msd_gen_outputs, msd_real_features, msd_gen_features,\n",
    "                                            real_mel_for_loss, gen_mel)\n",
    "            gen_loss.backward() # gradient 계산\n",
    "            gen_optim.step() # 가중치 업데이트\n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "            # Logging\n",
    "            if step % logging_step == 0:\n",
    "                print(f'| step: {step} | gen_loss: {gen_loss:4.3f} | mel_loss: {mel_loss:4.3f} | {time.time()-start:.3f} sec / {logging_step} steps |')\n",
    "                # Tensorboard에 기록\n",
    "                sw.add_scalar(\"training/gen_loss\", gen_loss, step)\n",
    "                sw.add_scalar(\"training/mel_loss\", mel_loss, step)\n",
    "                start = time.time() # start time 초기화\n",
    "            \n",
    "            # Validation\n",
    "            if step % validation_step == 0:\n",
    "                gen_model.eval()\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                mel_loss_sum = 0\n",
    "                with torch.no_grad():\n",
    "                    for i, batch in enumerate(val_dataloader):\n",
    "                        _, _, mel, _, wav, _ = batch\n",
    "                        real_mel = mel.to(device) # (1, F, 80)\n",
    "                        real_wav = wav.to(device) # (1, L)\n",
    "                        real_mel = real_mel.transpose(1, 2) # (1, 80, F)\n",
    "                        real_wav = real_wav.unsqueeze(1) # (1, 1, L)\n",
    "                        _, _, Max_L = real_wav.size()\n",
    "                        \n",
    "                        real_mel_for_loss = mel_spectrogram(real_wav.squeeze(1), fmax=11025)\n",
    "                        \n",
    "                        # Generator\n",
    "                        gen_wav = gen_model(real_mel) # (1, L*)\n",
    "                        _, _, Gen_L = gen_wav.size()\n",
    "                        if Max_L < Gen_L:\n",
    "                            gen_wav = gen_wav[:, :, :Max_L]\n",
    "                        else:\n",
    "                            real_wav = real_wav[:, :, :Gen_L]\n",
    "                        \n",
    "                        gen_mel = mel_spectrogram(gen_wav.squeeze(1), fmax=11025) # (1, 80, F)\n",
    "                        \n",
    "                        # Loss\n",
    "                        mel_loss = Mel_Spectrogram_Loss(real_mel_for_loss, gen_mel)\n",
    "                        mel_loss_sum += mel_loss\n",
    "                        \n",
    "                        if i <= 4: # 오디오와 그림은 5개 저장\n",
    "                            sw.add_audio(f'generated/gen_wav_{i}', gen_wav[0], step, sample_rate)\n",
    "                            sw.add_figure(f'generated/gen_mel_{i})',\n",
    "                                          plot_mel(gen_mel[0].detach().cpu().numpy(), step, mel_loss),\n",
    "                                          step)\n",
    "                        \n",
    "                    mel_loss_avg = mel_loss_sum / (i+1)\n",
    "                    sw.add_scalar(\"validation/mel_loss\", mel_loss_avg, step)\n",
    "                    print(f\"Validation mel_loss: {mel_loss_avg}\")\n",
    "                \n",
    "                gen_model.train()\n",
    "                start = time.time() # start time 초기화\n",
    "                        \n",
    "            # Checkpoint\n",
    "            if step % checkpoint_step == 0:\n",
    "                save_dir = './hifi_ckpt/' + model_name\n",
    "                torch.save({\n",
    "                    'gen_model': gen_model.state_dict(),\n",
    "                    'mpd_model': mpd_model.state_dict(),\n",
    "                    'msd_model': msd_model.state_dict(),\n",
    "                    'gen_optim': gen_optim.state_dict(),\n",
    "                    'disc_optim': disc_optim.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'step': step\n",
    "                }, os.path.join(save_dir, 'ckpt-{}.pt'.format(step)))\n",
    "                \n",
    "                start = time.time() # start time 초기화\n",
    "                \n",
    "        # Epoch 증가에 따라 scheduler 조정\n",
    "        gen_scheduler.step()\n",
    "        disc_scheduler.step()\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a7f57f",
   "metadata": {},
   "source": [
    "## 4.7. Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a5f764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Inference(text, model_name, check_step, hifi_model_name, hifi_check_step):\n",
    "    \"\"\"\n",
    "    (NEW) check_step = None\n",
    "    (LOAD) check_step: 불러오고자 하는 모델의 step\n",
    "    \n",
    "    학습된 Glow-TTS와 HiFi-GAN으로부터 목소리를 만들어냅니다.\n",
    "    text에는 text의 리스트가 입력됩니다.\n",
    "    (예) [\"안녕?\", \"반가워.\"]\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Define Models & Optimizer\n",
    "    model = GlowTTS().to(device)\n",
    "    gen_model = Generator().to(device) # HiFi-GAN\n",
    "\n",
    "    # Load Models & Optimizer\n",
    "    if check_step is not None:\n",
    "        os.makedirs('ckpt/' + model_name, exist_ok=True)\n",
    "        check_point = './ckpt/' + model_name + \"/ckpt-\" + str(check_step) + \".pt\"\n",
    "        ckpt = torch.load(check_point)\n",
    "        model.load_state_dict(ckpt['model'])\n",
    "        print(f'Load Glow-TTS model: {model_name} | Step {check_step}')\n",
    "        \n",
    "    if hifi_check_step is not None:\n",
    "        check_point = './hifi_ckpt/' + hifi_model_name + \"/ckpt-\" + str(hifi_check_step) + \".pt\"\n",
    "        ckpt = torch.load(check_point)\n",
    "        gen_model.load_state_dict(ckpt['gen_model'])\n",
    "        print(f'Load HiFi-GAN model: {hifi_model_name} | Step {hifi_check_step}')\n",
    "        \n",
    "    # Text Preprocessing\n",
    "    filters = '([,])'\n",
    "    text_list = []\n",
    "    text_len = []\n",
    "    for idx, s in enumerate(text):\n",
    "        # 문자열에서 정규표현식을 이용하여 특정 문자열을 필터링하고,\n",
    "        # 이를 빈 문자열('')로 대체한다.\n",
    "        sentence = re.sub(re.compile(filters), '', s)\n",
    "        sentence = text_to_sequence(sentence)\n",
    "        text_tensor = torch.tensor(sentence)\n",
    "        text_list.append(text_tensor)\n",
    "        text_len.append(len(text_tensor))\n",
    "\n",
    "    max_text_len = max(text_len)\n",
    "\n",
    "    # text zero_padding\n",
    "    text_batch = torch.zeros((len(text_list), max_text_len), dtype=torch.int32)\n",
    "    for i, x in enumerate(text_list):\n",
    "        text_batch[i, :len(x)] = torch.Tensor(x)\n",
    "    text = text_batch.to(device)\n",
    "\n",
    "    os.makedirs('./save', exist_ok=True)\n",
    "    \n",
    "    for idx in range(text.size(0)):\n",
    "        (y, z_mean, z_log_std, log_det, z_mask), (x_mean, x_log_std, x_mask), (attention_alignment, x_log_dur, log_d) = \\\n",
    "                            model(text[idx].unsqueeze(0), [text_len[idx]], inference=True)\n",
    "        gen_wav = gen_model(y)\n",
    "        gen_wav = gen_wav.cpu().detach().numpy()\n",
    "        sf.write('./save/'+'{}.wav'.format(idx), gen_wav[0][0], sample_rate)\n",
    "        print(f'Save: {idx}.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6693163",
   "metadata": {},
   "source": [
    "# 5. Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023ce1c3",
   "metadata": {},
   "source": [
    "## 5.1. Train HiFi-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c47e3424",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INPUT #####\n",
    "model_name = \"model_01\"\n",
    "check_step = 401000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a78669",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    HiFi_GAN_train(model_name, check_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e746d756",
   "metadata": {},
   "source": [
    "## 5.2. Train Glow-TTS using pretrained HiFi-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cee75270",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INPUT #####\n",
    "model_name = \"model_01\"\n",
    "check_step = 337000\n",
    "hifi_model_name = \"model_01\"\n",
    "hifi_check_step = 400000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49359b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if __name__ == \"__main__\":\n",
    "    Glow_TTS_train(model_name, check_step, hifi_model_name, hifi_check_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3a8470",
   "metadata": {},
   "source": [
    "## 5.3. Generate TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf6941f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INPUT #####\n",
    "model_name = \"model_sce+01\"\n",
    "check_step = 352000\n",
    "hifi_model_name = \"model_01\"\n",
    "hifi_check_step = 440000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ccad085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Glow-TTS model: model_sce+01 | Step 352000\n",
      "Load HiFi-GAN model: model_01 | Step 440000\n",
      "Save: 0.wav\n",
      "Save: 1.wav\n",
      "Save: 2.wav\n",
      "Save: 3.wav\n",
      "Save: 4.wav\n",
      "Save: 5.wav\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if __name__ == \"__main__\":\n",
    "    text = [\n",
    "        \"안녕하세요? 딥러닝 티티에스를 테스트하는 중입니다.\",\n",
    "        \"근데 너희 때도 운영체제 과제가 있었어?\",\n",
    "        \"더 최신의 모델을 찾거나 많은 실험을 할 수도 있고, 통계적 분석을 통해 최적의 데이터셋을 만들어서 좋은 티티에스 모델을 만들 수도 있습니다.\",\n",
    "        \"이 모델을 개발한다면 통화 내용을 글로 남김과 동시에 정확한 음성 전달이 가능하다는 장점이 있습니다.\",\n",
    "        \"이 모델을 개발한다면 통화 내용을 글로 남김과 동시에 정확한 음성 전달이 가능하다는 장점이 있습니다.\",\n",
    "        \"이 모델을 개발한다면 통화 내용을 글로 남김과 동시에 정확한 음성 전달이 가능하다는 장점이 있습니다.\",\n",
    "    ]\n",
    "    Inference(text, model_name, check_step, hifi_model_name, hifi_check_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5bf32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
